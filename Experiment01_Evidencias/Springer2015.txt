@Article{Gupta2009,
author="Gupta, Anita
and Li, Jingyue
and Conradi, Reidar
and R{\o}nneberg, Harald
and Landre, Einar",
title="A case study comparing defect profiles of a reused framework and of applications reusing it",
journal="Empirical Software Engineering",
year="2009",
month="Apr",
day="01",
volume="14",
number="2",
pages="227--255",
abstract="The benefits of software reuse have been studied for many years. Several previous studies have observed that reused software has a lower defect density than newly built software. However, few studies have investigated empirically the reasons for this phenomenon. To date, we have only the common sense observation that as software is reused over time, the fixed defects will accumulate and will result in high-quality software. This paper reports on an industrial case study in a large Norwegian Oil and Gas company, involving a reused Java class framework and two applications that use that framework. We analyzed all trouble reports from the use of the framework and the applications according to the Orthogonal Defect Classification (ODC), followed by a qualitative Root Cause Analysis (RCA). The results reveal that the framework has a much lower defect density in total than one application and a slightly higher defect density than the other. In addition, the defect densities of the most severe defects of the reused framework are similar to those of the applications that are reusing it. The results of the ODC and RCA analyses reveal that systematic reuse (i.e. clearly defined and stable requirements, better design, hesitance to change, and solid testing) lead to lower defect densities of the functional-type defects in the reused framework than in applications that are reusing it. However, the different ``nature'' of the framework and the applications (e.g. interaction with other software, number and complexity of business logic, and functionality of the software) may confound the causal relationship between systematic reuse and the lower defect density of the reused software. Using the results of the study as a basis, we present an improved overall cause--effect model between systematic reuse and lower defect density that will facilitate further studies and implementations of software reuse.",
issn="1573-7616",
doi="10.1007/s10664-008-9081-9",
url="https://doi.org/10.1007/s10664-008-9081-9"
}

@Article{Lindholm2014,
author="Lindholm, Christin
and Notander, Jesper Pedersen
and H{\"o}st, Martin",
title="A case study on software risk analysis and planning in medical device development",
journal="Software Quality Journal",
year="2014",
month="Sep",
day="01",
volume="22",
number="3",
pages="469--497",
abstract="Software failures in medical devices can lead to catastrophic situations. Therefore, it is crucial to handle software-related risks when developing medical devices, and there is a need for further analysis of how this type of risk management should be conducted. The objective of this paper is to collect and summarise experiences from conducting risk management with an organisation developing medical devices. Specific focus is put on the first steps of the risk management process, i.e. risk identification, risk analysis, and risk planning. The research is conducted as action research, with the aim of analysing and giving input to the organisation's introduction of a software risk management process. First, the method was defined based on already available methods and then used. The defined method focuses on user risks, based on scenarios describing the expected use of the medical device in its target environment. During the use of the method, different stakeholders, including intended users, were involved. Results from the case study show that there are challenging problems in the risk management process with respect to definition of the system boundary and system context, the use of scenarios as input to the risk identification, estimation of detectability during risk analysis, and action proposals during risk planning. It can be concluded that the risk management method has potential to be used in the development organisation, although future research is needed with respect to, for example, context limitation and how to allow for flexible updates of the product.",
issn="1573-1367",
doi="10.1007/s11219-013-9222-2",
url="https://doi.org/10.1007/s11219-013-9222-2"
}

@Article{Stelzer1997,
author="Stelzer, Dirk
and Mellis, Werner
and Herzwurm, Georg",
title="A critical look at ISO 9000 for software quality management",
journal="Software Quality Journal",
year="1997",
month="Jun",
day="01",
volume="6",
number="2",
pages="65--79",
abstract="A considerable number of software suppliers report improvements in product and service quality, development costs and time to market achieved with the help of the ISO 9000 standards. Nevertheless, the ISO 9000 family has received unfavourable criticism in journals, textbooks and at software quality conferences. The paper summarizes, discusses and reviews eleven of the most popular arguments against the ISO 9000 standards. The review of the criticism is based on findings of two empirical surveys among European software suppliers that have implemented an ISO 9000 quality system. The paper concludes with suggestions and guidelines for advances in software quality management concepts, such as the ISO 9000 family, CMM, BOOTSTRAP and the emerging SPICE standard.",
issn="1573-1367",
doi="10.1023/A:1018591430752",
url="https://doi.org/10.1023/A:1018591430752"
}

@Article{Li2007,
author="Li, Jingzhou
and Ruhe, Guenther
and Al-Emran, Ahmed
and Richter, Michael M.",
title="A flexible method for software effort estimation by analogy",
journal="Empirical Software Engineering",
year="2007",
month="Feb",
day="01",
volume="12",
number="1",
pages="65--106",
abstract="Effort estimation by analogy uses information from former similar projects to predict the effort for a new project. Existing analogy-based methods are limited by their inability to handle non-quantitative data and missing values. The accuracy of predictions needs improvement as well. In this paper, we propose a new flexible method called AQUA that is able to overcome the limitations of former methods. AQUA combines ideas from two known analogy-based estimation techniques: case-based reasoning and collaborative filtering. The method is applicable to predict effort related to any object at the requirement, feature, or project levels. Which are the main contributions of AQUA when compared to other methods? First, AQUA supports non-quantitative data by defining similarity measures for different data types. Second, it is able to tolerate missing values. Third, the results from an explorative study in this paper shows that the prediction accuracy is sensitive to both the number N of analogies (similar objects) taken for adaptation and the threshold T for the degree of similarity, which is true especially for larger data sets. A fixed and small number of analogies, as assumed in existing analogy-based methods, may not produce the best accuracy of prediction. Fourth, a flexible mechanism based on learning of existing data is proposed for determining the appropriate values of N and T likely to offer the best accuracy of prediction. New criteria to measure the quality of prediction are proposed. AQUA was validated against two internal and one public domain data sets with non-quantitative attributes and missing values. The obtained results are encouraging. In addition, acomparative analysis with existing analogy-based estimation methods was conducted using three publicly available data sets that were used by these methods. Intwo of the three cases, AQUA outperformed all other methods.",
issn="1573-7616",
doi="10.1007/s10664-006-7552-4",
url="https://doi.org/10.1007/s10664-006-7552-4"
}

@Article{Bilotta1998,
author="Bilotta, John G.
and McGrew, John F.",
title="A Guttman Scaling of CMM Level 2 Practices: Investigating the Implementation Sequences Underlying Software Engineering Maturity",
journal="Empirical Software Engineering",
year="1998",
month="Jun",
day="01",
volume="3",
number="2",
pages="159--177",
abstract="The Capability Maturity Model specifies several hundred key practices that must be implemented as a team moves from Level 1 to Level 5 of the model. However, the model does not specify within each level the optimal sequence in which to implement the practices. Level 2 contains 121 such practices grouped under six key process areas (KPAs) which are further subdivided into five common feature areas (CFAs). Although the KPA/CFA structure has a logical fit with the organizational structure of very large software development teams, it does not correspond to the reality of small teams. Using Level 2 audit data collected on 10 small software development teams, the authors try to determine whether the data itself can point to a more appropriate implementation strategy for small teams. The data is analyzed using Guttman scaling techniques (scalogram analysis). The results indicate that there is a single underlying, orderable dimension at Level 2 which lays out a step-by-step path upward from Level 1. The order of the items is found to map well to the familiar Plan-Do-Check-Act cycle widely used by project managers to organize and control work efforts. The extracted scale can be used as an assessment tool to provide management with a quick snapshot of a team's current position relative to Level 2.",
issn="1573-7616",
doi="10.1023/A:1008084231983",
url="https://doi.org/10.1023/A:1008084231983"
}

@Article{Funkhouser2008,
author="Funkhouser, Owen
and Etzkorn, Letha Hughes
and Hughes, William E.",
title="A lightweight approach to software validation by comparing UML use cases with internal program documentation selected via call graphs",
journal="Software Quality Journal",
year="2008",
month="Mar",
day="01",
volume="16",
number="1",
pages="131--156",
abstract="This research involves a methodology and associated proof of concept tool to partially automate software validation by comparing UML use cases with particular execution scenarios in source code. These execution scenarios are represented as the internal documentation (identifier names and comments) associated with sequences of execution in static call graphs. This methodology has the potential to reduce validation time and associated costs in many organizations, by enabling quick and easy validation of software relative to the use cases that describe the requirements. The proof of concept tool as it currently stands is intended as an aid to an IV{\&}V software engineer, to assist in directing the software validation process. The approach is lightweight and easily implemented.",
issn="1573-1367",
doi="10.1007/s11219-007-9034-3",
url="https://doi.org/10.1007/s11219-007-9034-3"
}

@Article{GalinacGrbac2015,
author="Galinac Grbac, Tihana
and Car, {\v{Z}}eljka
and Huljeni{\'{c}}, Darko",
title="A quality cost reduction model for large-scale software development",
journal="Software Quality Journal",
year="2015",
month="Jun",
day="01",
volume="23",
number="2",
pages="363--390",
abstract="Understanding quality costs is recognized as a prerequisite for decreasing the variability of the success of software development projects. This paper presents an empirical quality cost reduction (QCR) model to support the decision-making process for additional investment in the early phases of software verification. The main idea of the QCR model is to direct additional investment into software units that have some fault-slip potential in their later verification phases, with the aim of reducing costs and increasing product quality. The fault-slip potential of a software unit within a system is determined by analogy with historical projects. After a preliminary study on a sample of software units, which proves that we can lower quality costs with additional investment into particular verification activities, we examine the effectiveness of the proposed QCR model using real project data. The results show that applying the model produces a positive business case, meaning that the model lowers quality costs and increases quality, resulting in economic benefit. The potential to reduce quality costs is growing significantly with the evolution of software systems and the reuse of their software units. The proposed model is the result of a research project performed at Ericsson.",
issn="1573-1367",
doi="10.1007/s11219-014-9240-8",
url="https://doi.org/10.1007/s11219-014-9240-8"
}

@Article{Drago2015,
author="Drago, Mauro Luigi
and Ghezzi, Carlo
and Mirandola, Raffaela",
title="A quality driven extension to the QVT-relations transformation			language",
journal="Computer Science - Research and Development",
year="2015",
month="Feb",
day="01",
volume="30",
number="1",
pages="1--20",
abstract="An emerging approach to software development is Model Driven Software Development				(MDSD). It shifts the focus from source code to models, aims at cost reduction, risk				mitigation, and eases the engineering of complex applications. System models can be				used in the early development stages to verify certain relevant properties, such as				performance, before source code is available and problems become hard and costly to				solve. The present status of Model Driven Engineering (MDE) is still far from this				ideal situation. A well-known problem is feedback provisioning, which arises when				different solutions for the same design problem exist. An approach for feedback				provisioning automation leverages model transformations, which glue together models				in an MDSD setting, encapsulate the design rationale, and promote knowledge reuse				and solutions otherwise available only to experienced engineers. In this article we				present QVTR2, our solution to the feedback problem.					QVTR2 is an extension of the QVT-Relations language				with constructs to express design alternatives, their impact on non-functional				metrics, and how to evaluate them and guide the engineers in the selection of the				most appropriate solution. We demonstrate the effectiveness of our solution by using				the QVTR2 engine to perform a modified version of the				standard UML-to-RDBMS transformation in the				context of a real e-commerce application, and by showing how we can guide a				non-expert engineer in the selection of a solution that satisfies given performance				requirements.",
issn="1865-2042",
doi="10.1007/s00450-011-0202-0",
url="https://doi.org/10.1007/s00450-011-0202-0"
}

@Article{Pino2016,
author="Pino, Francisco J.
and Garc{\'i}a, Fel{\'i}x
and Piattini, Mario
and Oktaba, Hanna",
title="A research framework for building SPI proposals in small organizations: the COMPETISOFT experience",
journal="Software Quality Journal",
year="2016",
month="Sep",
day="01",
volume="24",
number="3",
pages="489--518",
abstract="Establishing a research strategy that is suitable for undertaking research on software engineering is vital if we are to guarantee that research products are developed and validated following a systematic and coherent method. We took this into account as we carried out the COMPETISOFT research project, which investigated software process improvement (SPI) in the context of Latin American small companies. That experience has enabled us to develop a research strategy based on the integrated use of action research and case study methods. This paper introduces the proposed research strategy and provides extensive discussion of its application for: (1) developing the Methodological framework of COMPETISOFT for SPI, (2) putting this framework into practice in eight small software companies, and (3) refine the Methodological framework due to the practice feedback. The use of this research strategy allowed us to observe that it was suitable for developing, refining, improving, applying, and validating COMPETISOFT's Methodology framework. Furthermore, having seen it applied, we believe that this strategy offers a successful integration of action research and case study, which can be useful for conducting research in other software engineering areas which address needs of small software companies.",
issn="1573-1367",
doi="10.1007/s11219-015-9278-2",
url="https://doi.org/10.1007/s11219-015-9278-2"
}

@Article{Patnayakuni2010,
author="Patnayakuni, Ravi
and Ruppel, Cynthia P.",
title="A socio-technical approach to improving the systems development process",
journal="Information Systems Frontiers",
year="2010",
month="Apr",
day="01",
volume="12",
number="2",
pages="219--234",
abstract="Research on improving the systems development processes has primarily focused on mechanisms such as tools, software development methodologies, knowledge sharing and process capabilities. This research has yielded considerable insights into improving the systems development process, but the large majority of information systems development projects still continue to be over budget, late, and ineffective in meeting user needs. Together with the advent of software development moving offshore, or consisting of offshore team members, a more holistic approach is appropriate. Approached from a socio-technical perspective the software development process is viewed as a process embedded in a social and a technical subsystem. Drawing upon socio-technical work design principles, this paper suggests how capabilities of the development process can be improved. Data collected from a survey of software development practices in organizations indicates that organizations at different levels of process capabilities differ in work system characteristics as well as process performance. For example, the use of multi-skilled teams was found to be significantly related to the systems development process maturity level as well as significantly related to all the performance measures studied. This paper provides empirical support for the socio-technical approach and provides a theoretical foundation for designing software process initiatives in organizations.",
issn="1572-9419",
doi="10.1007/s10796-008-9093-4",
url="https://doi.org/10.1007/s10796-008-9093-4"
}

@Article{Chu2012,
author="Chu, Peng-Hua
and Hsueh, Nien-Lin
and Chen, Hong-Hsiang
and Liu, Chien-Hung",
title="A test case refactoring approach for pattern-based software development",
journal="Software Quality Journal",
year="2012",
month="Mar",
day="01",
volume="20",
number="1",
pages="43--75",
abstract="In the current trend, Extreme Programing methodology is widely adopted by small and medium-sized projects for dealing with rapidly or indefinite changing requirements. Test-first strategy and code refactoring are the important practices of Extreme Programing for rapid development and quality support. The test-first strategy emphasizes that test cases are designed before system implementation to keep the correctness of artifacts during software development; whereas refactoring is the removal of ``bad smell'' code for improving quality without changing its semantics. However, the test-first strategy may conflict with code refactoring in the sense that the original test cases may be broken or inefficient for testing programs, which are revised by code refactoring. In general, the developers revise the test cases manually since it is not complicated. However, when the developers perform a pattern-based refactoring to improve the quality, the effort of revising the test cases is much more than that in simple code refactoring. In our observation, a pattern-based refactoring is composed of many simple and atomic code refactorings. If we have the composition relationship and the mapping rules between code refactoring and test case refactoring, we may infer a test case revision guideline in pattern-based refactoring. Based on this idea, in this research, we propose a four-phase approach to guide the construction of the test case refactoring for design patterns. We also introduce our approach by using some well-known design patterns and evaluate its feasibility by means of test coverage.",
issn="1573-1367",
doi="10.1007/s11219-011-9143-x",
url="https://doi.org/10.1007/s11219-011-9143-x"
}

@Article{Riungu-Kalliosaari2016,
author="Riungu-Kalliosaari, Leah
and Taipale, Ossi
and Smolander, Kari
and Richardson, Ita",
title="Adoption and use of cloud-based testing in practice",
journal="Software Quality Journal",
year="2016",
month="Jun",
day="01",
volume="24",
number="2",
pages="337--364",
abstract="This qualitative study addresses the adoption, utilization and effects of cloud-based testing in different organizational contexts. We approached the research problem by conducting thirty-five interviews with professionals in 20 organizations and applied grounded theory as the research method. The results indicate that cloud-based testing provides viable solutions to meet the testing needs within organizations. Cloud-based resources can be applied in performing various testing activities such as performance and multiplatform testing as well supporting practitioners in involving users during iterative development and testing. Cloud-based testing also adds value to practitioners by enabling easier management of the cloud-based testing resources and helping to produce improved end products. We use the results of the study to propose a strategy that can be used to assist practitioners in their decision-making processes towards adoption of cloud-based testing.",
issn="1573-1367",
doi="10.1007/s11219-014-9256-0",
url="https://doi.org/10.1007/s11219-014-9256-0"
}

@Article{Li2006,
author="Li, Jingyue
and Bj{\o}rnson, Finn Olav
and Conradi, Reidar
and Kampenes, Vigdis B.",
title="An empirical study of variations in COTS-based software development processes in the Norwegian IT industry",
journal="Empirical Software Engineering",
year="2006",
month="Sep",
day="01",
volume="11",
number="3",
pages="433--461",
abstract="More and more software projects use Commercial-Off-The-Shelf (COTS) components. Although previous studies have proposed specific COTS-based development processes, there are few empirical studies that investigate how to use and customize COTS-based development processes for different project contexts. This paper describes an exploratory study of state-of-the-practice of COTS-based development processes. Sixteen software projects in the Norwegian IT companies have been studied by structured interviews. The results are that COTS-specific activities can be successfully incorporated in most traditional development processes (such as waterfall or prototyping), given proper guidelines to reduce risks and provide specific assistance. We have identified four COTS-specific activities---the build vs. buy decision, COTS component selection, learning and understanding COTS components, and COTS component integration -- and one new role, that of a knowledge keeper. We have also found a special COTS component selection activity for unfamiliar components, combining Internet searches with hands-on trials. The process guidelines are expressed as scenarios, problems encountered, and examples of good practice. They can be used to customize the actual development processes, such as in which lifecycle phase to put the new activities into. Such customization crucially depends on the project context, such as previous familiarity with possible COTS components and flexibility of requirements.",
issn="1573-7616",
doi="10.1007/s10664-006-9005-5",
url="https://doi.org/10.1007/s10664-006-9005-5"
}

@Article{Šmite2014,
author="{\v{S}}mite, Darja
and Wohlin, Claes
and Galvi{\c{n}}a, Zane
and Prikladnicki, Rafael",
title="An empirically based terminology and taxonomy for global software engineering",
journal="Empirical Software Engineering",
year="2014",
month="Feb",
day="01",
volume="19",
number="1",
pages="105--153",
abstract="Many organizations nowadays strive for utilization of benefits offered by global software engineering (GSE) and sourcing strategies are thus discussed more often. Since there are so many variations of the attributes associated with global software projects a large amount of new terms has been introduced. The diversity in sourcing jargon however has caused difficulties in determining which term to use in which situation, and thus causing further obstacles to searching and finding relevant research during e.g. systematic literature reviews. The inability of judging the applicability of the research in an industrial context is another important implication on the transferability of research into practice. Thus the need for accurate terminology and definitions for different global sourcing situations emerges as a way for the community to build upon each other's work and hence making progress more quickly. In this paper we first investigate the state of the use of the GSE jargon concluding that terminology is very diverse (many synonyms used to describe the same phenomena), often confusing (same terms used to describe different phenomena) and occasionally ambiguous (few terms used to describe several phenomena). In order to address the identified problems, we conducted a Delphi-inspired study with ten well-established researchers in GSE and developed an empirically based glossary for the key concepts in global software engineering. We then propose a taxonomy for GSE by categorizing the selected terms based on generalization-specialization relationships and illustrate how the taxonomy can be used to categorize and map existing knowledge. The contribution targets future researchers, who will publish or synthesize further empirical work and practitioners, who are interested in published empirical cases. Therefore this work is expected to make a contribution to the future development of research in the GSE field, and alleviate understandability and transferability of existing and future knowledge into practice.",
issn="1573-7616",
doi="10.1007/s10664-012-9217-9",
url="https://doi.org/10.1007/s10664-012-9217-9"
}

@Article{Sharp2004,
author="Sharp, Helen
and Robinson, Hugh",
title="An Ethnographic Study of XP Practice",
journal="Empirical Software Engineering",
year="2004",
month="Dec",
day="01",
volume="9",
number="4",
pages="353--375",
abstract="Agile methods are a response to more rigorous and traditional approaches to software development which are perceived to have failed both customers and software development practitioners. eXtreme Programming (XP) is an example agile method and we report on an ethnographic study of XP practice carried out in a small company developing web-based intelligent advertisements. We identify five characterizing themes within XP practice and summarize these findings in terms of XP culture.",
issn="1573-7616",
doi="10.1023/B:EMSE.0000039884.79385.54",
url="https://doi.org/10.1023/B:EMSE.0000039884.79385.54"
}

@Article{Lormans2008,
author="Lormans, Marco
and van Deursen, Arie
and Gross, Hans-Gerhard",
title="An industrial case study in reconstructing requirements views",
journal="Empirical Software Engineering",
year="2008",
month="Dec",
day="01",
volume="13",
number="6",
pages="727--760",
abstract="Requirements views, such as coverage and status views, are an important asset for monitoring and managing software development projects. We have developed a method that automates the process of reconstructing these views, and we have built a tool, ReqAnalyst, that supports this method. This paper presents an investigation as to which extent requirements views can be automatically generated in order to monitor requirements in industrial practice. The paper focuses on monitoring the requirements in test categories and test cases. In order to retrieve the necessary data, an information retrieval technique, called Latent Semantic Indexing, was used. The method was applied in an industrial study. A number of requirements views were defined and experiments were carried out with different reconstruction settings for generating these views. Finally, we explored how these views can help the developers during the software development process.",
issn="1573-7616",
doi="10.1007/s10664-008-9078-4",
url="https://doi.org/10.1007/s10664-008-9078-4"
}

@Article{Damian2004,
author="Damian, Daniela
and Zowghi, Didar
and Vaidyanathasamy, Lakshminarayanan
and Pal, Yogendra",
title="An Industrial Case Study of Immediate Benefits of Requirements Engineering Process Improvement at the Australian Center for Unisys Software",
journal="Empirical Software Engineering",
year="2004",
month="Mar",
day="01",
volume="9",
number="1",
pages="45--75",
abstract="This paper describes an industrial experience in process improvement at one of the Unisys development labs in Australia. Following a capability maturity model (CMM) mini-assessment, the organization is undertaking significant changes in the requirements management process, which include the introduction of group session approaches to requirements analysis and a structured method for writing requirements. An empirical evaluation which investigated other aspects of the process improvement than the CMM model indicates tangible benefits as well as perceived long-term benefits during design and testing. Findings confirm that a more thorough requirements analysis results in more clearly defined, better understood and specified requirements, and an enhanced ability to address the market needs and product strategy requirements. The catalyst behind these improvements included project management leadership, managing the human dimension, collaboration among stakeholders and senior management support.",
issn="1573-7616",
doi="10.1023/B:EMSE.0000013514.19567.ad",
url="https://doi.org/10.1023/B:EMSE.0000013514.19567.ad"
}

@Article{Ahmed2010,
author="Ahmed, Faheem
and Capretz, Luiz Fernando",
title="An organizational maturity model of software product line engineering",
journal="Software Quality Journal",
year="2010",
month="Jun",
day="01",
volume="18",
number="2",
pages="195--225",
abstract="Software product line engineering is an inter-disciplinary concept. It spans the dimensions of business, architecture, process, and the organization. Some of the potential benefits of this approach include cost reduction, improvements in product quality and a decrease in product development time. The increasing popularity of software product line engineering in the software industry necessitates a process maturity evaluation methodology. Accordingly, this paper presents an organizational maturity model of software product line engineering for evaluating the maturity of organizational dimension. The model assumes that organizational theories, behavior, and management play a critical role in the institutionalization of software product line engineering within an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective and design of the questionnaires are to collect information about the software product line engineering process from the dual perspectives of organizational behavior and management. Furthermore, we conducted two case studies and reported the assessment results using the organizational maturity model presented in this paper.",
issn="1573-1367",
doi="10.1007/s11219-009-9088-5",
url="https://doi.org/10.1007/s11219-009-9088-5"
}

@Article{Christie1998,
author="Christie, Alan M.
and Kellner, Marc I.
and Riddle, William E.",
title="An Overview of Process Technology Activities at the Software Engineering Institute",
journal="Journal of Systems Integration",
year="1998",
month="May",
day="01",
volume="8",
number="2",
pages="121--132",
abstract="This paper provides an overview of the rationale for, and direction of the process technology work being pursued at the Software Engineering Institute. The paper then describes some of the activities that the SEI has recently been involved in: process definition, process simulation, and technology for collaborative process support.",
issn="1573-8787",
doi="10.1023/A:1008206119511",
url="https://doi.org/10.1023/A:1008206119511"
}

@Article{Reis2002,
author="Reis, Rodrigo Quites
and Reis, Carla Alessandra Lima
and Schlebbe, Heribert
and Jos{\'e} nunes, Daltro",
title="Automatic Verification of Static Policies on Software Process Models",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="197--234",
abstract="Software Process Technology evolved to support software processes management by assisting the modeling, enacting and evolution of complex process models. This paper presents a contribution to this field, describing a mechanism to formally model Static Policies, which are useful to automate the verification of user-defined process syntactical properties. The proposed mechanism acts during software process modeling, promoting the reuse of policy instances across different processes in a software organization. In this text, the language for Static Policy definition is presented first through its informal description, followed by examples. This paper also discusses some of the main issues related to the formal (algebraic) semantics defined for the Policy interpreter, which based the implementation of a Java-based prototype.",
issn="1573-7489",
doi="10.1023/A:1020509809235",
url="https://doi.org/10.1023/A:1020509809235"
}

@Article{Jönsson2006,
author="J{\"o}nsson, Per
and Wohlin, Claes",
title="Benchmarking k-nearest neighbour imputation with homogeneous Likert data",
journal="Empirical Software Engineering",
year="2006",
month="May",
day="31",
volume="11",
number="3",
pages="463",
abstract="Missing data are common in surveys regardless of research field, undermining statistical analyses and biasing results. One solution is to use an imputation method, which recovers missing data by estimating replacement values. Previously, we have evaluated the hot-deck k-Nearest Neighbour (k-NN) method with Likert data in a software engineering context. In this paper, we extend the evaluation by benchmarking the method against four other imputation methods: Random Draw Substitution, Random Imputation, Median Imputation and Mode Imputation. By simulating both non-response and imputation, we obtain comparable performance measures for all methods. We discuss the performance of k-NN in the light of the other methods, but also for different values of k, different proportions of missing data, different neighbour selection strategies and different numbers of data attributes. Our results show that the k-NN method performs well, even when much data are missing, but has strong competition from both Median Imputation and Mode Imputation for our particular data. However, unlike these methods, k-NN has better performance with more data attributes. We suggest that a suitable value of k is approximately the square root of the number of complete cases, and that letting certain incomplete cases qualify as neighbours boosts the imputation ability of the method.",
issn="1573-7616",
doi="10.1007/s10664-006-9001-9",
url="https://doi.org/10.1007/s10664-006-9001-9"
}

@Article{Love1998,
author="Love, Matthew
and Siddiqi, Jawed",
title="Can the case for CASE technology be advanced by Process Improvement?",
journal="Software Quality Journal",
year="1998",
month="Mar",
day="01",
volume="7",
number="1",
pages="3--10",
abstract="Findings of a case study that focused on understanding how software development proceeded in a small Information Systems Department (ISD) located within a major public sector service are presented. The observations collected relate to a period prior to, during and after the introduction of a CASE (Computer Aided Software Engineering) tool. They are based on a combination of minutes from quality circle meetings, interviews and regular on-site observations by the first author. The study used a framework loosely based on SEI's CMM model to characterize the state of practice before and after the introduction of the tool and to assess process improvement.",
issn="1573-1367",
doi="10.1023/B:SQJO.0000042055.40508.32",
url="https://doi.org/10.1023/B:SQJO.0000042055.40508.32"
}

@Inbook{Humphrey1992,
author="Humphrey, Watts S.",
editor="Yeh, Raymond T.",
title="CASE Planning and the Software Process",
bookTitle="Case Technology: A Special Issue of the Journal of Systems Integration",
year="1992",
publisher="Springer US",
address="Boston, MA",
pages="59--75",
abstract="Automating a software process magnifies its strengths and accentuates its weaknesses. Automation can make an effective process more effective, but it can also make a chaotic process even worse---and at considerable expense. Anyone who buys expensive tools to solve an ill-defined problem is likely to be disappointed. Unless such tools are obtained as part of a thoughtful software process improvement plan, the purchase could be an expensive mistake. This article discusses software process maturity and its relationship to planning and installing computer-aided software engineering (CASE) systems. Although process is not a magic answer (there isn't one), the key issues are discussed from a process perspective, and guidelines are given for avoiding the most common pitfalls. Because CASE systems can involve significant investment, an economic justification may be necessary. The relevant financial considerations are therefore discussed, and some basic steps for producing such justifications are outlined. Finally, some key considerations for introducing and using CASE systems are discussed.",
isbn="978-1-4615-3644-4",
doi="10.1007/978-1-4615-3644-4_4",
url="https://doi.org/10.1007/978-1-4615-3644-4_4"
}

@Article{Bjarnason2014,
author="Bjarnason, Elizabeth
and Runeson, Per
and Borg, Markus
and Unterkalmsteiner, Michael
and Engstr{\"o}m, Emelie
and Regnell, Bj{\"o}rn
and Sabaliauskaite, Giedre
and Loconsole, Annabella
and Gorschek, Tony
and Feldt, Robert",
title="Challenges and practices in aligning requirements with verification and validation: a case study of six companies",
journal="Empirical Software Engineering",
year="2014",
month="Dec",
day="01",
volume="19",
number="6",
pages="1809--1855",
abstract="Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.",
issn="1573-7616",
doi="10.1007/s10664-013-9263-y",
url="https://doi.org/10.1007/s10664-013-9263-y"
}

@Article{Benediktsson2003,
author="Benediktsson, Oddur
and Dalcher, Darren
and Reed, Karl
and Woodman, Mark",
title="COCOMO-Based Effort Estimation for Iterative and Incremental Software Development",
journal="Software Quality Journal",
year="2003",
month="Nov",
day="01",
volume="11",
number="4",
pages="265--281",
abstract="Incremental software development and delivery have been used in software projects in many ways for many years. Justifications for incremental approaches include risk amelioration, the management of evolving requirements, and end-user involvement. Incremental development, including iterative, incremental delivery, has become a norm in many sectors. However, there has been little work on modelling the effort in such development and hence a dearth of comparative analyses of cost models for incremental development/delivery. We attempt to rectify this by proposing a COCOMO-style effort model for incremental development/delivery and explore the relationship between effort and the number of increments, thereby providing new insights into the economic impact of incremental approaches to software projects.",
issn="1573-1367",
doi="10.1023/A:1025809010217",
url="https://doi.org/10.1023/A:1025809010217"
}

@Article{Chatterji2016,
author="Chatterji, Debarshi
and Carver, Jeffrey C.
and Kraft, Nicholas A.",
title="Code clones and developer behavior: results of two surveys of the clone research community",
journal="Empirical Software Engineering",
year="2016",
month="Aug",
day="01",
volume="21",
number="4",
pages="1476--1508",
abstract="The literature presents conflicting claims regarding the effects of clones on software maintainability. For a community to progress, it is important to identify and address those areas of disagreement. Many claims, such as those related to developer behavior, either lack human-based empirical validation or are contradicted by other studies. This paper describes the results of two surveys to evaluate the level of agreement among clone researchers regarding claims that have not yet been validated through human-based empirical study. The surveys covered three key clone-related research topics: general information, developer behavior, and evolution. Survey 1 focused on high-level information about all three topics, whereas Survey 2 focused specifically on developer behavior. Approximately 20 clone researchers responded to each survey. The survey responses showed a lack of agreement on some major clone-related topics. First, the respondents disagree about the definitions of clone types, with some indicating the need for a taxonomy based upon developer intent. Second, the respondents were uncertain whether the ratio of cloned to non-cloned code affected system quality. Finally, the respondents disagree about the usefulness of various detection, analysis, evolution, and visualization tools for clone management tasks such as tracking and refactoring of clones. The overall results indicate the need for more focused, human-based empirical research regarding the effects of clones during maintenance. The paper proposes a strategy for future research regarding developer behavior and code clones in order to bridge the gap between clone research and the application of that research in clone maintenance.",
issn="1573-7616",
doi="10.1007/s10664-015-9394-4",
url="https://doi.org/10.1007/s10664-015-9394-4"
}

@Article{Karahasanoviæ2005,
author="Karahasanovi{\ae}, Amela
and Anda, Bente
and Arisholm, Erik
and Hove, Siw Elisabeth
and J{\o}rgensen, Magne
and Sj{\o}berg, Dag I. K.
and Welland, Ray",
title="Collecting Feedback during Software Engineering Experiments",
journal="Empirical Software Engineering",
year="2005",
month="Apr",
day="01",
volume="10",
number="2",
pages="113--147",
abstract="Objective: To improve the qualitative data obtained from software engineering experiments by gathering feedback during experiments. Rationale: Existing techniques for collecting quantitative and qualitative data from software engineering experiments do not provide sufficient information to validate or explain all our results. Therefore, we would like a cost-effective and unobtrusive method of collecting feedback from subjects during an experiment to augment other sources of data. Design of study: We formulated a set of qualitative questions that might be answered by collecting feedback during software engineering experiments. We then developed a tool to collect such feedback from experimental subjects. This feedback-collection tool was used in four different experiments and we evaluated the usefulness of the feedback obtained in the context of each experiment. The feedback data was triangulated with other sources of quantitative and qualitative data collected for the experiments. Results: We have demonstrated that the collection of feedback during experiments provides useful additional data to: validate the data obtained from other sources about solution times and quality of solutions; check process conformance; understand problem solving processes; identify problems with experiments; and understand subjects' perception of experiments. Conclusions: Feedback collection has proved useful in four experiments and we intend to use the feedback-collection tool in a range of other experiments to further explore the cost-effectiveness and limitations of this technique. It is also necessary to carry out a systematic study to more fully understand the impact of the feedback-collecting tool on subjects' performance in experiments.",
issn="1573-7616",
doi="10.1007/s10664-004-6189-4",
url="https://doi.org/10.1007/s10664-004-6189-4"
}

@Article{Abrahamsson2002,
author="Abrahamsson, Pekka",
title="Commitment Nets in Software Process Improvement",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="407--438",
abstract="Several studies have revealed the fact that nearly two-thirds of all software process improvement (SPI) efforts have failed or have at least fallen short of expectations. Literature and practice have shown that commitment to SPI at all organizational levels is essential for the success of any SPI endeavor. A research model for studying the existence, development and interplay of SPI-related commitment is introduced in this paper. This study suggests that software organizations operate through strategic, operational and personal commitment nets. These nets consist of actors, drivers, concerns, actions, commitment, and outcomes. The commitment nets model is applied in a study of four industrial SPI initiatives. The results from two of these cases are reported here. The results show that SPI is driven through the formation and reformation of commitment nets. The contents of strategic, operational and personal commitment nets are laid out and implications are discussed.",
issn="1573-7489",
doi="10.1023/A:1020526329708",
url="https://doi.org/10.1023/A:1020526329708"
}

@Article{Shin2016,
author="Shin, Donghwan
and Jee, Eunkyoung
and Bae, Doo-Hwan",
title="Comprehensive analysis of FBD test coverage criteria using mutants",
journal="Software {\&} Systems Modeling",
year="2016",
month="Jul",
day="01",
volume="15",
number="3",
pages="631--645",
abstract="Function block diagram (FBD), a graphical modeling language for programmable logic controllers, has been widely used to implement safety critical system software such as nuclear reactor protection systems. With the growing importance of structural testing for FBD models, structural test coverage criteria for FBD models have been proposed and evaluated using mutation analysis in our previous work. We extend the previous work by comprehensively analyzing the relationships among fault detection effectiveness, test suite size, and coverage level through several research questions. We generate a large number of test suites achieving an FBD test coverage ranging from 0 to 100 {\%}, and we also generate many artificial faults (i.e. mutants) for the FBD models. Our analysis results show that the fault detection effectiveness of the FBD coverage criteria increases with increasing coverage levels, and the coverage criteria are highly effective at detecting faults in all subject models. Furthermore, the test suites generated with the FBD coverage criteria are more effective and efficient than the randomly generated test suites. The FBD coverage criteria are strong at detecting faults in Boolean edges, while relatively weak at detecting wrong constants in FBD models. Empirical knowledge regarding our experiments provide the validity of using the FBD coverage criteria, and therefore, of FBD model-based testing.",
issn="1619-1374",
doi="10.1007/s10270-014-0428-y",
url="https://doi.org/10.1007/s10270-014-0428-y"
}

@Article{Gasston1999,
author="Gasston, Jennifer
and Halloran, Pat",
title="Continuous Software Process Improvement Requires Organisational Learning: An Australian Case Study",
journal="Software Quality Journal",
year="1999",
month="Sep",
day="01",
volume="8",
number="1",
pages="37--51",
abstract="The study reported in this paper suggests that in order to achieve optimal benefits from implementing process improvement programs, organisations must move towards becoming what is termed ``a learning organisation.'' Software process assessment ``leads to the identification and selection of key activities for improvement and the continuous application of improvements to match business needs'' (ISO/IEC 1996). Continuous improvement requires a commitment to learning on the part of the organisation (Garvin 1993). A model to help identify evidence of learning (the Organisational Learning Evaluation Cycle [OLEC] has been developed and empirically tested in the study. We have found evidence to suggest that the case study organisation had not moved through all three of Garvin's (1993) overlapping phases of organisational learning and as a result the firm's improvement program did not achieve optimal benefits for the organisation. The paper concludes by discussing why significant improvement in performance was not achieved.",
issn="1573-1367",
doi="10.1023/A:1008974818812",
url="https://doi.org/10.1023/A:1008974818812"
}

@Article{Dittrich2008,
author="Dittrich, Yvonne
and R{\"o}nkk{\"o}, Kari
and Eriksson, Jeanette
and Hansson, Christina
and Lindeberg, Olle",
title="Cooperative method development",
journal="Empirical Software Engineering",
year="2008",
month="Jun",
day="01",
volume="13",
number="3",
pages="231--260",
abstract="The development of methods tools and process improvements is best to be based on the understanding of the development practice to be supported. Qualitative research has been proposed as a method for understanding the social and cooperative aspects of software development. However, qualitative research is not easily combined with the improvement orientation of an engineering discipline. During the last 6 years, we have applied an approach we call `cooperative method development', which combines qualitative social science fieldwork, with problem-oriented method, technique and process improvement. The action research based approach focusing on shop floor software development practices allows an understanding of how contextual contingencies influence the deployment and applicability of methods, processes and techniques. This article summarizes the experiences and discusses the further development of this approach based on several research projects in cooperation with industrial partners.",
issn="1573-7616",
doi="10.1007/s10664-007-9057-1",
url="https://doi.org/10.1007/s10664-007-9057-1"
}

@Article{Prikladnicki2014,
author="Prikladnicki, Rafael
and Boden, Alexander
and Avram, Gabriela
and de Souza, Cleidson R. B.
and Wulf, Volker",
title="Data collection in global software engineering research: learning from past experience",
journal="Empirical Software Engineering",
year="2014",
month="Aug",
day="01",
volume="19",
number="4",
pages="822--856",
abstract="Global Software Engineering has become a standard in today's software industry. Research in distributed software development poses severe challenges that are due to the spatial and temporal distribution of the actors, as well as to language, intercultural and organizational aspects. These challenges occur in addition to ``traditional'' challenges of the domain itself in large-scale software projects, like coordination and communication issues, requirements volatily, lack of domain knowledge, among others. While several authors have reported empirical studies of global software development projects, the methodological difficulties and challenges of this type of studies have not been sufficiently discussed. In this paper, we share our experiences of collecting and analysing qualitative data in the context of Global Software Engineering projects. We discuss strategies for gaining access to field sites, building trust and documenting distributed and complex work practices in the context of several research projects we have conducted in the past 9 years. The experiences described in this paper illustrate the need to deal with fundamental problems, such as understanding local languages and different cultures, observing synchronous interaction, or dealing with barriers imposed by political conflicts between the sites. Based on our findings, we discuss some practical implications and strategies that can be used by other researchers and provide some recommendations for future research in methodological aspects of Global Software Engineering.",
issn="1573-7616",
doi="10.1007/s10664-012-9240-x",
url="https://doi.org/10.1007/s10664-012-9240-x"
}

@Article{Bowes2014,
author="Bowes, David
and Hall, Tracy
and Gray, David",
title="DConfusion: a technique to allow cross study performance evaluation of fault prediction studies",
journal="Automated Software Engineering",
year="2014",
month="Apr",
day="01",
volume="21",
number="2",
pages="287--313",
abstract="There are many hundreds of fault prediction models published in the literature. The predictive performance of these models is often reported using a variety of different measures. Most performance measures are not directly comparable. This lack of comparability means that it is often difficult to evaluate the performance of one model against another. Our aim is to present an approach that allows other researchers and practitioners to transform many performance measures back into a confusion matrix. Once performance is expressed in a confusion matrix alternative preferred performance measures can then be derived. Our approach has enabled us to compare the performance of 600 models published in 42 studies. We demonstrate the application of our approach on 8 case studies, and discuss the advantages and implications of doing this.",
issn="1573-7535",
doi="10.1007/s10515-013-0129-8",
url="https://doi.org/10.1007/s10515-013-0129-8"
}

@Article{Osei-Bryson2008,
author="Osei-Bryson, Kweku-Muata
and Ngwenyama, Ojelanki",
title="Decision models for information systems management",
journal="Information Systems Frontiers",
year="2008",
month="Jul",
day="01",
volume="10",
number="3",
pages="277--279",
issn="1572-9419",
doi="10.1007/s10796-008-9082-7",
url="https://doi.org/10.1007/s10796-008-9082-7"
}

@Article{Giraudo2003,
author="Giraudo, Griselda
and Tonella, Paolo",
title="Designing and Conducting an Empirical Study on Test Management Automation",
journal="Empirical Software Engineering",
year="2003",
month="Mar",
day="01",
volume="8",
number="1",
pages="59--81",
abstract="Test management aims at organizing, documenting and executing test cases, and at generating execution reports. The adoption of a support tool and of a standard process for such activities is expected to improve the current practice. ITALO is a European project devoted to the evaluation of the benefits coming from test management automation. In this paper the experiences collected and the lessons learned during ITALO are summarized. A formal methodology was adopted for the selection of a support tool among those available from the market. A survey of the current practice in component testing was conducted to adapt the existing process model so as to obtain the greatest benefits from automation. An empirical study was then designed to measure the effects that are expected to be produced by the new test process complemented with the introduction of the support tool. Three pilot projects were conducted to measure the benefits obtained from tool usage and process modification. Results are presented and discussed in this paper.",
issn="1573-7616",
doi="10.1023/A:1021720916127",
url="https://doi.org/10.1023/A:1021720916127"
}

@Article{Hoda2012,
author="Hoda, Rashina
and Noble, James
and Marshall, Stuart",
title="Developing a grounded theory to explain the practices of self-organizing Agile teams",
journal="Empirical Software Engineering",
year="2012",
month="Dec",
day="01",
volume="17",
number="6",
pages="609--639",
abstract="Software Engineering researchers are constantly looking to improve the quantity and quality of their research findings through the use of an appropriate research methodology. Over the last decade, there has been a sustained increase in the number of researchers exploring the human and social aspects of Software Engineering, many of whom have used Grounded Theory. We have used Grounded Theory as a qualitative research method to study 40 Agile practitioners across 16 software organizations in New Zealand and India and explore how these Agile teams self-organize. We use our study to demonstrate the application of Grounded Theory to Software Engineering. In doing so, we present (a) a detailed description of the Grounded Theory methodology in general and its application in our research in particular; (b) discuss the major challenges we encountered while performing Grounded Theory's various activities and our strategies for overcoming these challenges; and (c) we present a sample of our data and results to illustrate the artifacts and outcomes of Grounded Theory research.",
issn="1573-7616",
doi="10.1007/s10664-011-9161-0",
url="https://doi.org/10.1007/s10664-011-9161-0"
}

@Article{Ebert2002,
author="Ebert, Christof
and De Man, Jozef",
title="e-R{\&}D -- Effectively Managing Process Diversity",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="73--91",
abstract="Managing process diversity becomes increasingly relevant in software development. Software organizations typically do not work on the greenfield and thus need to integrate external workflows with R{\&}D internal workflow management and heterogeneous development and maintenance processes. To stay competitive with its software development, Alcatel has put in place an orchestrated improvement program of its processes and the underlying engineering tools environment. Why do we call this ``e-R{\&}D''? For two reasons. These improvement activities necessarily fit into the wider context of Alcatel's business process improvement and corporate e-business initiatives. The ``e-R{\&}D'' also means enabling of interactive R{\&}D processes and increasing collaborative work across the globe. At Alcatel we realized, during a substantial reengineering of our development and industrialization processes, that the approach to acquire an off-the-shelf process and tailor it to our needs was not applicable. Different processes need to be seamlessly integrated to avoid inconsistencies and inefficiency caused by replicated work. Specific focus is given within this article on how we manage process diversity in a product line where various components are embedded in individual architectures, asking for different but defined development and maintenance processes depending on pre-selected criteria.",
issn="1573-7489",
doi="10.1023/A:1020545406509",
url="https://doi.org/10.1023/A:1020545406509"
}

@Article{Šmite2010,
author="{\v{S}}mite, Darja
and Wohlin, Claes
and Gorschek, Tony
and Feldt, Robert",
title="Empirical evidence in global software engineering: a systematic review",
journal="Empirical Software Engineering",
year="2010",
month="Feb",
day="01",
volume="15",
number="1",
pages="91--118",
abstract="Recognized as one of the trends of the 21st century, globalization of the world economies brought significant changes to nearly all industries, and in particular it includes software development. Many companies started global software engineering (GSE) to benefit from cheaper, faster and better development of software systems, products and services. However, empirical studies indicate that achieving these benefits is not an easy task. Here, we report our findings from investigating empirical evidence in GSE-related research literature. By conducting a systematic review we observe that the GSE field is still immature. The amount of empirical studies is relatively small. The majority of the studies represent problem-oriented reports focusing on different aspects of GSE management rather than in-depth analysis of solutions for example in terms of useful practices or techniques. Companies are still driven by cost reduction strategies, and at the same time, the most frequently discussed recommendations indicate a necessity of investments in travelling and socialization. Thus, at the same time as development goes global there is an ambition to minimize geographical, temporal and cultural separation. These are normally integral parts of cross-border collaboration. In summary, the systematic review results in several descriptive classifications of the papers on empirical studies in GSE and also reports on some best practices identified from literature.",
issn="1573-7616",
doi="10.1007/s10664-009-9123-y",
url="https://doi.org/10.1007/s10664-009-9123-y"
}

@Article{Siakas2002,
author="Siakas, Kerstin V.
and Georgiadou, Elli",
title="Empirical Measurement of the Effects of Cultural Diversity on Software Quality Management",
journal="Software Quality Journal",
year="2002",
month="Sep",
day="01",
volume="10",
number="2",
pages="169--180",
abstract="The difficulties of achieving social acceptance for Software Quality Management systems have been underestimated in the past, and they will be exacerbated in the future by the globalization of the software market and the increasing use of cross-cultural development teams within multinational companies. Management that can take account of the cultural context of their endeavours will improve understanding, minimize risk and ensure a higher degree of success in improvement programs within the software industry.",
issn="1573-1367",
doi="10.1023/A:1020528024624",
url="https://doi.org/10.1023/A:1020528024624"
}

@Article{Raffo1999,
author="Raffo, David
and Kaltio, Timo
and Partridge, Derek
and Phalp, Keith
and Ramil, Juan F.",
title="Empirical Studies Applied to Software Process Models",
journal="Empirical Software Engineering",
year="1999",
month="Dec",
day="01",
volume="4",
number="4",
pages="353--369",
issn="1573-7616",
doi="10.1023/A:1009817721252",
url="https://doi.org/10.1023/A:1009817721252"
}

@Article{Gholami2014,
author="Gholami, Mahdi Fahmideh
and Sharifi, Mohsen
and Jamshidi, Pooyan",
title="Enhancing the OPEN Process Framework with service-oriented method fragments",
journal="Software {\&} Systems Modeling",
year="2014",
month="Feb",
day="01",
volume="13",
number="1",
pages="361--390",
abstract="Service orientation is a promising paradigm that enables the engineering of large-scale distributed software systems using rigorous software development processes. The existing problem is that every service-oriented software development project often requires a customized development process that provides specific service-oriented software engineering tasks in support of requirements unique to that project. To resolve this problem and allow situational method engineering, we have defined a set of method fragments in support of the engineering of the project-specific service-oriented software development processes. We have derived the proposed method fragments from the recurring features of 11 prominent service-oriented software development methodologies using a systematic mining approach. We have added these new fragments to the repository of OPEN Process Framework to make them available to software engineers as reusable fragments using this well-known method repository.",
issn="1619-1374",
doi="10.1007/s10270-011-0222-z",
url="https://doi.org/10.1007/s10270-011-0222-z"
}

@Article{Williams2014,
author="Williams, Byron J.
and Carver, Jeffrey C.",
title="Examination of the software architecture change characterization scheme using three empirical studies",
journal="Empirical Software Engineering",
year="2014",
month="Jun",
day="01",
volume="19",
number="3",
pages="419--464",
abstract="Software maintenance is one of the most crucial aspects of software development. Software engineering researchers must develop practical solutions to handle the challenges presented in maintaining mature software systems. Research that addresses practical means of mitigating the risks involved when changing software, reducing the complexity of mature software systems, and eliminating the introduction of preventable bugs is paramount to today's software engineering discipline. The Software Architecture Change Characterization Scheme (SACCS) provides software maintainers with a systematic approach to analyzing and characterizing the impact of a change prior to its implementation. SACCS was designed to help novice developers understand change requests, facilitate discussion among developers, and provide a higher-quality change compared with an ad hoc approach. In addition, this paper describes three controlled experiments designed to assess the viability of using SACCS and its ability to fulfill its goals. The successive studies build upon each other to enable progressive insights into the viability of the scheme. The results indicate that SACCS: 1) provides insight into the difficulty of a change request by assisting novice developers to consider various aspects of the request's potential to impact the system, 2) helps to facilitate discussion among developers by providing a common tool for change assessment, and 3) is a useful tool for supporting change implementation. The three experiments provide insight into the usefulness of SACCS, motivate additional research questions, and serve as a baseline for moving forward with research and further development of the approach.",
issn="1573-7616",
doi="10.1007/s10664-012-9223-y",
url="https://doi.org/10.1007/s10664-012-9223-y"
}

@Article{Lindvall2007,
author="Lindvall, Mikael
and Rus, Ioana
and Donzelli, Paolo
and Memon, Atif
and Zelkowitz, Marvin
and Betin-Can, Aysu
and Bultan, Tevfik
and Ackermann, Chris
and Anders, Bettina
and Asgari, Sima
and Basili, Victor
and Hochstein, Lorin
and Fellmann, J{\"o}rg
and Shull, Forrest
and Tvedt, Roseanne
and Pech, Daniel
and Hirschbach, Daniel",
title="Experimenting with software testbeds for evaluating new technologies",
journal="Empirical Software Engineering",
year="2007",
month="Aug",
day="01",
volume="12",
number="4",
pages="417--444",
abstract="The evolution of a new technology depends upon a good theoretical basis for developing the technology, as well as upon its experimental validation. In order to provide for this experimentation, we have investigated the creation of a software testbed and the feasibility of using the same testbed for experimenting with a broad set of technologies. The testbed is a set of programs, data, and supporting documentation that allows researchers to test their new technology on a standard software platform. An important component of this testbed is the Unified Model of Dependability (UMD), which was used to elicit dependability requirements for the testbed software. With a collection of seeded faults and known issues of the target system, we are able to determine if a new technology is adept at uncovering defects or providing other aids proposed by its developers. In this paper, we present the Tactical Separation Assisted Flight Environment (TSAFE) testbed environment for which we modeled and evaluated dependability requirements and defined faults to be seeded for experimentation. We describe two completed experiments that we conducted on the testbed. The first experiment studies a technology that identifies architectural violations and evaluates its ability to detect the violations. The second experiment studies model checking as part of design for verification. We conclude by describing ongoing experimental work studying testing, using the same testbed. Our conclusion is that even though these three experiments are very different in terms of the studied technology, using and re-using the same testbed is beneficial and cost effective.",
issn="1573-7616",
doi="10.1007/s10664-006-9034-0",
url="https://doi.org/10.1007/s10664-006-9034-0"
}

@Article{Koch2008,
author="Koch, Stefan",
title="Exploring the effects of SourceForge.net coordination and communication tools on the efficiency of open source projects using data envelopment analysis",
journal="Empirical Software Engineering",
year="2008",
month="Aug",
day="15",
volume="14",
number="4",
pages="397",
abstract="In this paper we explore possible benefits of communication and coordination tools in open source projects using an efficiency score derived from data envelopment analysis (DEA) as dependent variable. DEA is a general non-parametric method for efficiency comparisons without asking the user to define any relations between different factors or a production function. The method can account for economies or diseconomies of scale, and is able to deal with multi-input, multi-output systems in which the factors have different scales. Using two different data sets, successful and random open source projects, retrieved from SourceForge.net, we analyze impacts on their efficiency from the usage of communication and coordination tools. The results were mixed with no clear positive effects being proven consistently: In the data set of successful projects, mostly negative influences were found. On the contrary, tool adoption showed positive relationships to efficiency in the random data set. This stresses the importance of development status as a moderating variable and might also hint at threshold values for tool benefits. In addition, adoption of tools outside the hosting platform may be more likely for successful projects.",
issn="1573-7616",
doi="10.1007/s10664-008-9086-4",
url="https://doi.org/10.1007/s10664-008-9086-4"
}

@Article{Lepmets2012,
author="Lepmets, Marion
and Cater-Steel, Aileen
and Gacenga, Francis
and Ras, Eric",
title="Extending the IT service quality measurement framework through a systematic literature review",
journal="Journal of Service Science Research",
year="2012",
month="Jun",
day="01",
volume="4",
number="1",
pages="7--47",
abstract="Continuous improvement of service quality results in enhanced customer satisfaction, increased efficiency and maximisation of business value of the service within the company. Decision-making on the course of service quality improvement is based on the current status of the measurable service quality attributes. The aim of the paper is to describe the IT service quality attributes that could be measured to improve IT service quality. We report on a systematic literature review of IT service quality measurement. The review was based on 134 relevant journal articles related to IT service quality management. Of these, 91 articles were selected for analysis. We propose a detailed and comprehensive quality measurement framework for IT services using the results of the systematic literature review to extend previous work. The framework presents six common issue areas with their associated measurement categories, measures, and indicators. IT service providers can choose the measures to satisfy their specific information needs from the proposed IT service quality measurement framework. We conclude that IT service quality improvement efforts could benefit from considering the internal IT service quality attributes from the viewpoint of the value the provided IT service could bring to both the customer and the provider.",
issn="2093-0739",
doi="10.1007/s12927-012-0001-6",
url="https://doi.org/10.1007/s12927-012-0001-6"
}

@Article{Osterweil2009,
author="Osterweil, Leon J.",
title="Formalisms to Support the Definition of Processes",
journal="Journal of Computer Science and Technology",
year="2009",
month="Mar",
day="01",
volume="24",
number="2",
pages="198--211",
abstract="This paper emphasizes the importance of defining processes rigorously, completely, clearly, and in detail in order to support the complex projects that are essential to the modern world. The paper argues that such process definitions provide needed structure and context for the development of effective software systems. The centrality of process is argued by enumerating seven key ways in which processes and their definitions are expected to provide important benefits to society. The paper provides an example of a process formalism that makes good progress towards the difficult goal of being simultaneously rigorous, detailed, broad, and clear. Early experience suggests that these four key characteristics of this formalism do indeed seem to help it to support meeting the seven key benefits sought from process definitions. Additional research is suggested in order to gain more insights into needs in the area of process definition formalisms.",
issn="1860-4749",
doi="10.1007/s11390-009-9218-3",
url="https://doi.org/10.1007/s11390-009-9218-3"
}

@Article{Saiedian1996,
author="Saiedian, Hossein
and McClanahan, Laura M.",
title="Frameworks for quality software process: SEI Capability Maturity Model versus ISO 9000",
journal="Software Quality Journal",
year="1996",
month="Mar",
day="01",
volume="5",
number="1",
pages="1--23",
abstract="With the historical characterization of software development as being costly due to massive schedule delays, incorporation of the ever-changing technology, budget reductions, and missing customer requirements, the trend of the 1990s in establishing a quality improvement or a quality assurance programme has been over-whelming. The two popular models or frameworks for assessment of a quality assurance programme are the US government-sponsored Capability Maturity Model (CMM) and the internationally recognized ISO-9000 quality standards. Both of these two frameworks share a common concern regarding software quality and process management. Since it is not clear which of these two frameworks is most effective in achieving their shared objectives, it is valuable and timely to provide an objective overview of both models and to compare and contrast their features for quality software development. Because there are many legitimate areas for comparison, we have selected the two most important as a basis for comparison: (1) the role of management, and (2) the application of measurements. We also provide a summary of the reported impact of these two models on the organizations adhering to their standards, and include our observations and analysis.",
issn="1573-1367",
doi="10.1007/BF02420941",
url="https://doi.org/10.1007/BF02420941"
}

@Article{Moe2014,
author="Moe, Nils Brede
and {\v{S}}mite, Darja
and Hanssen, Geir Kjetil
and Barney, Hamish",
title="From offshore outsourcing to insourcing and partnerships: four failed outsourcing attempts",
journal="Empirical Software Engineering",
year="2014",
month="Oct",
day="01",
volume="19",
number="5",
pages="1225--1258",
abstract="Most large software companies are involved in offshore development, now small- and medium-sized companies are starting to undertake global sourcing too. Empirical research suggests that offshoring is not always successful; however, only a few comprehensive failure stories have been reported. The objective of our study has been to understand why small and medium-sized companies terminate their offshore outsourcing relationships and what alternative arrangements they undertake afterwards. Therefore, we designed a multiple case study of four medium-sized Scandinavian software companies that have terminated their offshore outsourcing relationships. Our results are based on data collected through semi-structured interviews, informal dialogues and analysis of company documents. We found that all companies terminated their offshore contracts because of low quality of the software being developed. This was caused by an inability to build the necessary human and social capital. The companies reported challenges with domain knowledge, a lack of commitment of external developers, cultural clashes, poor communication and high turnover, which only amplified the problems. After termination all four companies changed their sourcing strategy from offshore outsourcing to offshore insourcing and partnerships. We conclude that successful offshore software development requires a change from a cost-driven focus to an intellectual capital-driven focus. To prevent continuous investments into contracts that are destined to fail, companies should look for signs of escalating commitments and terminate relationships that cannot be corrected. Those companies that choose outsourcing shall also take into account that mismatch between the size of the offshore contract relative to the vendor may have a negative effect on a relationship.",
issn="1573-7616",
doi="10.1007/s10664-013-9272-x",
url="https://doi.org/10.1007/s10664-013-9272-x"
}

@Article{Beirne1997,
author="Beirne, Martin
and Panteli, Androniki
and Ramsay, Harvie",
title="Going soft on quality?:Process management in the Scottish software industry",
journal="Software Quality Journal",
year="1997",
month="Sep",
day="01",
volume="6",
number="3",
pages="195--209",
abstract="Despite their proliferation within the industry, quality assurance standards are insufficient to ensure effective software development. Scottish software companies are now highly sensitive to limitations imposed by the standards movement, flagging the need for more explicit attention to work organization and human resource issues. This article examines efforts to broaden the focus of quality management within the software industry, charting the impact of theoretical and practical initiatives and identifying the mediating role of commercial pressures, the consultancy marketplace and, crucially, staff reactions.",
issn="1573-1367",
doi="10.1023/A:1018564019365",
url="https://doi.org/10.1023/A:1018564019365"
}

@Article{Braun2014,
author="Braun, Peter
and Broy, Manfred
and Houdek, Frank
and Kirchmayr, Matthias
and M{\"u}ller, Mark
and Penzenstadler, Birgit
and Pohl, Klaus
and Weyer, Thorsten",
title="Guiding requirements engineering for software-intensive embedded systems in the automotive industry",
journal="Computer Science - Research and Development",
year="2014",
month="Feb",
day="01",
volume="29",
number="1",
pages="21--43",
abstract="Over the past decade, a dramatic increase of functionality, quantity, size, and complexity of software-intensive embedded systems in the automotive industry can be observed. In particular, the growing complexity drives current requirements engineering practices to the limits. In close cooperation between partners from industry and academia, the recently completed REMsES (Requirements Engineering and Management for software-intensive Embedded Systems) project has developed a guideline to support requirements engineering processes in the automotive industry. The guideline enables the requirements engineers to cope with the challenges that arise due to quantity, size and complexity of software-intensive systems. This article presents the major results of the project, namely, the fundamental principles of the approach, the guideline itself, the tool support, and the major findings obtained during the evaluation of the approach.",
issn="1865-2042",
doi="10.1007/s00450-010-0136-y",
url="https://doi.org/10.1007/s00450-010-0136-y"
}

@Article{Jabangwe2015,
author="Jabangwe, Ronald
and B{\"o}rstler, J{\"u}rgen
and Petersen, Kai",
title="Handover of managerial responsibilities in global software development: a case study of source code evolution and quality",
journal="Software Quality Journal",
year="2015",
month="Dec",
day="01",
volume="23",
number="4",
pages="539--566",
abstract="Studies report on the negative effect on quality in global software development (GSD) due to communication and coordination-related challenges. However, empirical studies reporting on the magnitude of the effect are scarce. This paper presents findings from an embedded explanatory case study on the change in quality over time, across multiple releases, for products that were developed in a GSD setting. The GSD setting involved periods of distributed development between geographically dispersed sites as well as a handover of project management responsibilities between the involved sites. Investigations were performed on two medium-sized products from a company that is part of a large multinational corporation. Quality is investigated quantitatively using defect data and measures that quantify two source code properties, size and complexity. Observations were triangulated with subjective views from company representatives. There were no observable indications that the distribution of work or handover of project management responsibilities had an impact on quality on both products. Among the product-, process- and people-related success factors, we identified well-designed product architectures, early handover planning and support from the sending site to the receiving site after the handover and skilled employees at the involved sites. Overall, these results can be useful input for decision-makers who are considering distributing development work between globally dispersed sites or handing over project management responsibilities from one site to another. Moreover, our study shows that analyzing the evolution of size and complexity properties of a product's source code can provide valuable information to support decision-making during similar projects. Finally, the strategy used by the company to relocate responsibilities can also be considered as an alternative for software transfers, which have been linked with a decline in efficiency, productivity and quality.",
issn="1573-1367",
doi="10.1007/s11219-014-9247-1",
url="https://doi.org/10.1007/s11219-014-9247-1"
}

@Article{Baldassarre2012,
author="Baldassarre, Maria Teresa
and Caivano, Danilo
and Pino, Francisco J.
and Piattini, Mario
and Visaggio, Giuseppe",
title="Harmonization of ISO/IEC 9001:2000 and CMMI-DEV: from a theoretical comparison to a real case application",
journal="Software Quality Journal",
year="2012",
month="Jun",
day="01",
volume="20",
number="2",
pages="309--335",
abstract="In the past years, both industrial and research communities in Software Engineering have shown special interest in Software Process Improvement---SPI. This is evidenced by the growing number of publications on the topic. The literature offers numerous quality frameworks for addressing SPI practices, which may be classified into two groups: ones that describe ``what'' should be done (ISO 9001, CMMI) and ones that describe ``how'' it should be done (Six Sigma, Goal Question Metrics-GQM). When organizations decide to adopt improvement initiatives, many models may be implied, each leveraging the best practices provided, in the quest to address the improvement challenges as well as possible. This may at the same time, however, generate confusion and overlapping activities, as well as extra effort and cost. That, in turn, risks generating a series of inefficiencies and redundancies that end up leading to losses rather than to effective process improvement. Consequently, it is important to move toward a harmonization of quality frameworks, aiming to identify intersections and overlapping parts, as well as to create a multi-model improvement solution. Our aim in this work is twofold: first of all, we propose a theoretical harmonization process that supports organizations interested in introducing quality management and software development practices or concerned about improving those they already have. This is done with specific reference to CMMI-DEV and ISO 9001 models in the direction ``ISO to CMMI-DEV'', showing how GQM is used to define operational goals that address ISO 9001 statements, reusable in CMMI appraisals. Secondly, we apply the theoretical comparison process to a real case, i.e., a Small Enterprise certified ISO 9001.",
issn="1573-1367",
doi="10.1007/s11219-011-9154-7",
url="https://doi.org/10.1007/s11219-011-9154-7"
}

@Article{GarcíaGuzmán2010,
author="Garc{\'i}a Guzm{\'a}n, Javier
and Salda{\~{n}}a Ramos, Javier
and Amescua Seco, Antonio
and Sanz Esteban, Ana",
title="How to get mature global virtual teams: a framework to improve team process management in distributed software teams",
journal="Software Quality Journal",
year="2010",
month="Dec",
day="01",
volume="18",
number="4",
pages="409--435",
abstract="Managing global software development teams is not an easy task because of the additional problems and complexities that have to be taken into account. This paper defines VTManager, a methodology that provides a set of efficient practices for global virtual team management in software development projects. These practices integrate software development techniques in global environments with others such as explicit practices for global virtual team management, definition of skills and abilities needed to work in these teams, availability of collaborative work environments and shared knowledge management practices. The results obtained and the lessons learned from implementing VTManager in a pilot project to develop software tools for collaborative work in rural environments are also presented. This project was carried out by geographically distributed teams involving people from seven countries with a high level of virtualness.",
issn="1573-1367",
doi="10.1007/s11219-010-9096-5",
url="https://doi.org/10.1007/s11219-010-9096-5"
}

@Article{Ebert1999,
author="Ebert, Christof
and Liedtke, Thomas
and Baisch, Ekkehard",
title="Improving reliability of large software systems",
journal="Annals of Software Engineering",
year="1999",
month="Feb",
day="01",
volume="8",
number="1",
pages="3--51",
abstract="Improving field performance of telecommunication systems is the key objective of both telecom suppliers and operators, as an increasing amount of business critical systems worldwide are relying on dependable telecommunication. Early defect detection improves field performance in terms of reduced field failure rates and reduced intrinsic downtime. This paper describes an integrated approach to improve early defect detection and thus field reliability of telecommunication switching systems. The assumptions at the start of the projects discussed in this paper are: Wide application of code inspections and thorough module testing must lead to a lower fault detection density in subsequent phases. At the same time criteria for selecting the most critical components for code reviews, code inspections and module test are provided in order to optimize efficiency. The primary goal is to identify critical components and to make failure predictions as early as possible during the life cycle and hence reduce managerial risk combined with too early or too late release of such a system to the field. During test release time prediction and field performance prediction are both based on tailored and superposed ENHPP reliability models. Experiences from projects of Alcatel's Switching and Routing Division are included to show practical impacts.",
issn="1573-7489",
doi="10.1023/A:1018971212809",
url="https://doi.org/10.1023/A:1018971212809"
}

@Article{Cocchio1999,
author="Cocchio, Luisa
and Puttero, Davide",
title="Industrial Requirements for Distributed SCM Tool",
journal="Software Quality Journal",
year="1999",
month="Oct",
day="01",
volume="8",
number="2",
pages="111--120",
abstract="As software development comes to be viewed more and more as an engineering discipline, Software Configuration Management is increasingly recognised as a key technology in the development of software. The Esprit VISCOUNT project1 aim is to implement an innovative Software Configuration Management (SCM) tool to use in a geographically distributed software development environment, a Virtual Software Corporation (VSC).",
issn="1573-1367",
doi="10.1023/A:1008952809919",
url="https://doi.org/10.1023/A:1008952809919"
}

@Article{Samoilenko2008,
author="Samoilenko, Sergey",
title="Information systems fitness and risk in IS development: Insights and implications from chaos and complex systems theories",
journal="Information Systems Frontiers",
year="2008",
month="May",
day="13",
volume="10",
number="3",
pages="281",
abstract="Information Systems Development often takes place within a complex and uncertain socio-technical environment. The use of methodologies is considered an appropriate way of reducing risks of failures of the ISD projects, allowing for managing of the complexity of the process and product of ISD. Traditional functionalist methodologies, however, are not adequately equipped for dealing with non-linear interactions endemic to such complex social systems as IS. This paper examines IS development from the perspective of the complex systems behavior and chaos theory. It offers insights and implications for augmenting traditional approaches to ISD that could lead to better strategies for managing complexities in the ISD process and the behavior of an IS.",
issn="1572-9419",
doi="10.1007/s10796-008-9078-3",
url="https://doi.org/10.1007/s10796-008-9078-3"
}

@Article{Karlström2006,
author="Karlstr{\"o}m, Daniel
and Runeson, Per",
title="Integrating agile software development into stage-gate managed product development",
journal="Empirical Software Engineering",
year="2006",
month="Jun",
day="01",
volume="11",
number="2",
pages="203--225",
abstract="Agile methods have evolved as a bottom-up approach to software development. However, as the software in embedded products is only one part of development projects, agile methods must coexist with project management models typically of the stage-gate type. This paper presents a qualitative case study of two large independent software system projects that have used eXtreme Programming (XP) for software development within contexts of stage-gate project management models. The study is comprised of open ended interviews with managers as well as practitioners, followed by a structured, fully traceable, qualitative analysis. We conclude that it is possible to integrate XP in a gate model context. Key issues for success are the interfaces towards the agile subproject and management attitudes towards the agile approach.",
issn="1573-7616",
doi="10.1007/s10664-006-6402-8",
url="https://doi.org/10.1007/s10664-006-6402-8"
}

@Article{Maurer2002,
author="Maurer, Frank
and Holz, Harald",
title="Integrating Process Support and Knowledge Management for Virtual Software Development Teams",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="145--168",
abstract="In this paper we describe how knowledge management and software process support can be integrated to improve the efficiency of virtual software teams. The approach presented here integrates a process enactment environment with an on-demand knowledge delivery strategy that is based on parameterized information needs models. The parameters in the information needs models are bound at project execution time to values extracted from the process enactment engine. Thus, the approach supports virtual teams by establishing a platform for systematic and task-specific knowledge exchange. The proposed approach is prototypically implemented in the MILOS system, an open source project of the University of Calgary (Canada) and the University of Kaiserslautern (Germany).",
issn="1573-7489",
doi="10.1023/A:1020505708326",
url="https://doi.org/10.1023/A:1020505708326"
}

@Article{Felderer2014,
author="Felderer, Michael
and Ramler, Rudolf",
title="Integrating risk-based testing in industrial test processes",
journal="Software Quality Journal",
year="2014",
month="Sep",
day="01",
volume="22",
number="3",
pages="543--575",
abstract="Risk-based testing has a high potential to improve the software development and test process as it helps to optimize the allocation of resources and provides decision support for the management. But for many organizations, its integration into an existing test process is a challenging task. In this article, we provide a comprehensive overview of existing work and present a generic testing methodology enhancing an established test process to address risks. On this basis, we develop a procedure on how risk-based testing can be introduced in a test process and derive a stage model for its integration. We then evaluate our approach for introducing risk-based testing by means of an industrial study and discuss benefits, prerequisites and challenges to introduce it. Potential benefits of risk-based testing identified in the studied project are faster detection of defects resulting in an earlier release, a more reliable release quality statement as well as the involved test-process optimization. As necessary prerequisites for risk-based testing, we identified an inhomogeneous distribution of risks associated with the various parts of the tested software system as well as consolidated technical and business views on it. Finally, the identified challenges of introducing risk-based testing are reliable risk assessment in the context of complex systems, the availability of experts for risk assessment as well as established tool supports for test management.",
issn="1573-1367",
doi="10.1007/s11219-013-9226-y",
url="https://doi.org/10.1007/s11219-013-9226-y"
}

@Article{Humphrey1995,
author="Humphrey, Watts S.",
title="Introducing the personal software process",
journal="Annals of Software Engineering",
year="1995",
month="Dec",
day="01",
volume="1",
number="1",
pages="311--325",
abstract="The personal software process (PSP) has been developed by the Software Engineering Institute (SEI) to address the improvement needs of individual software engineers. It should also be of help to small projects and modest-sized software organizations. In a one-semester graduate-level course, engineers are introduced to measurement, planning, and quality control methods. A defined sequence of programming exercises are used to illustrate these methods and the exercise data are used to provide the students with feedback on their performance. The PSP course has been taught at five universities at both the graduate and senior undergraduate level. Various PSP introduction methods have also been tried at four industrial organizations, but the course format has also been found most effective. Results to date indicate that PSP training motivates engineers to use disciplined methods and it helps them to achieve significant quality and productivity improvements. While PSP principles have been demonstrated with software engineering students, their effectiveness has not yet been measured in industrial practice. This paper describes the PSP process, the strategic approach and experience to date.",
issn="1573-7489",
doi="10.1007/BF02249055",
url="https://doi.org/10.1007/BF02249055"
}

@Article{Jaliff1997,
author="Jaliff, J. S.
and Jonsson, C. {\AA}.
and Ljungblad, A.",
title="Introduction of modern software technology. A case from Swedish industry",
journal="Software Quality Journal",
year="1997",
month="Mar",
day="01",
volume="6",
number="1",
pages="1--11",
abstract="A project to develop a new system, Core Master 2, was undertaken in Sweden in 1990. This is a relatively large system, for nuclear power plant core engineering. Main project goals were to increase end-user productivity while reducing user-induced errors and software life-cycle costs. To meet them and improve software quality, object-oriented technology was adopted. The software process became much slower than expected. Learning aspects related to the new technology and changing specifications account for most of the delay. The first version is now ready and is being shipped to customers. Lessons learned from the process, education and training are discussed",
issn="1573-1367",
doi="10.1023/A:1018536726634",
url="https://doi.org/10.1023/A:1018536726634"
}

@Article{Tarhan2011,
author="Tarhan, Ayca
and Demirors, Onur",
title="Investigating the effect of variations in the test development process: a case from a safety-critical system",
journal="Software Quality Journal",
year="2011",
month="Dec",
day="01",
volume="19",
number="4",
pages="615--642",
abstract="Variation is inherent to a process, and process management demands understanding the nature of variation in quantitative terms, for evaluation and prediction purposes. This understanding requires the identification of process indicators that build the system of variation. To utilize quantitative techniques to understand and improve a software process, more indicators are needed than in a manufacturing process. The need to identify the indicators of a software process and the lack of a generic approach to assess the ability of a software process for quantitative management encouraged us to carry out a sequence of studies that resulted in the development of an Assessment Approach for Quantitative Process Management (A2QPM). This paper explains an application of the A2QPM to the test development process of an avionics software project and presents the results. The study aimed at understanding the effect of the test design stage and the effect of internal reviews as verification activities in test development, with respect to process productivity and product quality measures. The measurement data collected during the execution of the processes were analyzed by control charts to observe the evidence of process stability. The mean values of measurement data were utilized to make performance comparisons between the various executions of the test development process. The results showed that process productivity was unaffected, but the test procedure quality was positively influenced by the application of test design and internal reviews. The utilization of the A2QPM as a guide for the quantitative implementation enabled the systematic evaluation of the test development process and measures prior to analysis. This resulted in the identification of process clusters having stable variation.",
issn="1573-1367",
doi="10.1007/s11219-011-9129-8",
url="https://doi.org/10.1007/s11219-011-9129-8"
}

@Article{Sfetsos2006,
author="Sfetsos, Panagiotis
and Angelis, Lefteris
and Stamelos, Ioannis",
title="Investigating the extreme programming system--An empirical study",
journal="Empirical Software Engineering",
year="2006",
month="Jun",
day="01",
volume="11",
number="2",
pages="269--301",
abstract="In this paper we discuss our empirical study about the advantages and difficulties 15 Greek software companies experienced applying Extreme Programming (XP) as a holistic system in software development. Based on a generic XP system including feedback influences and using a cause-effect model including social-technical affecting factors, as our research tool, the study statistically evaluates the application of XP practices in the software companies being studied. Data were collected from 30 managers and developers, using the sample survey technique with questionnaires and interviews, in a time period of six months. Practices were analysed individually, using Descriptive Statistics (DS), and as a whole by building up different models using stepwise Discriminant Analysis (DA). The results have shown that companies, facing various problems with common code ownership, on-site customer, 40--hour week and metaphor, prefer to develop their own tailored XP method and way of working-practices that met their requirements. Pair programming and test-driven development were found to be the most significant success factors. Interactions and hidden dependencies for the majority of the practices as well as communication and synergy between skilled personnel were found to be other significant success factors. The contribution of this preliminary research work is to provide some evidence that may assist companies in evaluating whether the XP system as a holistic framework would suit their current situation.",
issn="1573-7616",
doi="10.1007/s10664-006-6404-6",
url="https://doi.org/10.1007/s10664-006-6404-6"
}

@Article{Kocaguneli2013,
author="Kocaguneli, Ekrem
and Menzies, Tim
and Keung, Jacky W.",
title="Kernel methods for software effort estimation",
journal="Empirical Software Engineering",
year="2013",
month="Feb",
day="01",
volume="18",
number="1",
pages="1--24",
abstract="Analogy based estimation (ABE) generates an effort estimate for a new software project through adaptation of similar past projects (a.k.a. analogies). Majority of ABE methods follow uniform weighting in adaptation procedure. In this research we investigated non-uniform weighting through kernel density estimation. After an extensive experimentation of 19 datasets, 3 evaluation criteria, 5 kernels, 5 bandwidth values and a total of 2090 ABE variants, we found that: (1) non-uniform weighting through kernel methods cannot outperform uniform weighting ABE and (2) kernel type and bandwidth parameters do not produce a definite effect on estimation performance. In summary simple ABE approaches are able to perform better than much more complex approaches. Hence,---provided that similar experimental settings are adopted---we discourage the use of kernel methods as a weighting strategy in ABE.",
issn="1573-7616",
doi="10.1007/s10664-011-9189-1",
url="https://doi.org/10.1007/s10664-011-9189-1"
}

@Article{Khoshkbarforoushha2011,
author="Khoshkbarforoushha, Alireza
and Jamshidi, Pooyan
and Nikravesh, Ali
and Shams, Fereidoon",
title="Metrics for BPEL process context-independency analysis",
journal="Service Oriented Computing and Applications",
year="2011",
month="Sep",
day="01",
volume="5",
number="3",
pages="139--157",
abstract="BPEL processes are workflow-oriented composite services for service-oriented solutions. Rapidly changing environment and turbulent market conditions require flexible BPEL processes to adapt with several modifications during their life cycles. Such adaptability and flexibility require the low degree of dependency or coupling between a BPEL process and its surrounding environment. In fact, heavy coupling and context dependency with partners provoke several undesirable drawbacks such as poor understandability, inflexibility, inadaptability, and defects. This paper is to propose metrics at the design phase to measure BPEL process context independency. With the aid of these metrics, the architect could analyze and control the context independency of a BPEL process quantitatively. To validate the metrics, authors collected a data set consisting 70 BPEL processes and also gathered the expert's rating of context independency through conducting a controlled experiment. The obtained results reveal that there exists a high statistical correlation between the proposed metrics and the expert's judgment of context independency.",
issn="1863-2394",
doi="10.1007/s11761-011-0077-8",
url="https://doi.org/10.1007/s11761-011-0077-8"
}

@Article{Lee2014,
author="Lee, Taeho
and Gu, Taewan
and Baik, Jongmoon",
title="MND-SCEMP: an empirical study of a software cost estimation modeling process in the defense domain",
journal="Empirical Software Engineering",
year="2014",
month="Feb",
day="01",
volume="19",
number="1",
pages="213--240",
abstract="The primary focus of weapon systems research and development has moved from a hardware base to a software base and the cost of software development is increasing gradually. An accurate estimation of the cost of software development is now a very important task in the defense domain. However, existing models and tools for software cost estimation are not suitable for the defense domain due to problems of accuracy. Thus, it is necessary to develop cost estimation models that are appropriate to specific domains. Furthermore, most studies of methodology development are aligned with generic methodologies that do not consider the pertinent factors to specific domains, whereas new methodologies should reflect specific domains. In this study, we apply two generic methodologies to the development of a software cost estimation model, before suggesting an integrated modeling process specifically for the national defense domain. To validate our proposed modeling process, we performed an empirical study of 113 software development projects on weapon systems in Korea. A software cost estimation model was developed by applying the proposed modeling process. The MMRE value of this model was 0.566 while the accuracy was appropriate for use. We conclude that the modeling process and software cost estimation model developed in this study is suitable for estimating resource requirements during weapon system development in South Korea's national defense domain. This modeling process and model may facilitate more accurate resource estimation by project planners, which will lead to more successful project execution.",
issn="1573-7616",
doi="10.1007/s10664-012-9220-1",
url="https://doi.org/10.1007/s10664-012-9220-1"
}

@Article{Hall2007,
author="Hall, Tracy
and Jagielska, Dorota
and Baddoo, Nathan",
title="Motivating developer performance to improve project outcomes in a high maturity organization",
journal="Software Quality Journal",
year="2007",
month="Dec",
day="01",
volume="15",
number="4",
pages="365--381",
abstract="In this paper we discuss the impact software developer performance has on project outcomes. Project performance remains unreliable in the software industry with many compromised software systems reported in the press. We investigate the impact that developer performance has on aspects of project success and explore how developer performance is motivated. We present interview, focus group and questionnaire data collected from a team of developers working in a software organization that has been assessed at CMM level 5. Our main findings are that developers value technical skills in their colleagues, but appreciate these especially when supplemented with good human skills. Software developers with a proactive, flexible, adaptable approach who are prepared to share knowledge and follow good practice are said to be the best developers. Motivators for these developers are pay and benefits, recognition and opportunities for achievement in their work. Overall, we found that technical competence, interpersonal skills and adherence to good practices are thought to have the biggest impact on software project success.",
issn="1573-1367",
doi="10.1007/s11219-007-9028-1",
url="https://doi.org/10.1007/s11219-007-9028-1"
}

@Article{Mäntylä2015,
author="M{\"a}ntyl{\"a}, Mika V.
and Adams, Bram
and Khomh, Foutse
and Engstr{\"o}m, Emelie
and Petersen, Kai",
title="On rapid releases and software testing: a case study and a semi-systematic literature review",
journal="Empirical Software Engineering",
year="2015",
month="Oct",
day="01",
volume="20",
number="5",
pages="1384--1425",
abstract="Large open and closed source organizations like Google, Facebook and Mozilla are migrating their products towards rapid releases. While this allows faster time-to-market and user feedback, it also implies less time for testing and bug fixing. Since initial research results indeed show that rapid releases fix proportionally less reported bugs than traditional releases, this paper investigates the changes in software testing effort after moving to rapid releases in the context of a case study on Mozilla Firefox, and performs a semi-systematic literature review. The case study analyzes the results of 312,502 execution runs of the 1,547 mostly manual system-level test cases of Mozilla Firefox from 2006 to 2012 (5 major traditional and 9 major rapid releases), and triangulates our findings with a Mozilla QA engineer. We find that rapid releases have a narrower test scope that enables a deeper investigation of the features and regressions with the highest risk. Furthermore, rapid releases make testing more continuous and have proportionally smaller spikes before the main release. However, rapid releases make it more difficult to build a large testing community , and they decrease test suite diversity and make testing more deadline oriented. In addition, our semi-systematic literature review presents the benefits, problems and enablers of rapid releases from 24 papers found using systematic search queries and a similar amount of papers found through other means. The literature review shows that rapid releases are a prevalent industrial practice that are utilized even in some highly critical domains of software engineering, and that rapid releases originated from several software development methodologies such as agile, open source, lean and internet-speed software development. However, empirical studies proving evidence of the claimed advantages and disadvantages of rapid releases are scarce.",
issn="1573-7616",
doi="10.1007/s10664-014-9338-4",
url="https://doi.org/10.1007/s10664-014-9338-4"
}

@Article{Seo2013,
author="Seo, Yeong-Seok
and Bae, Doo-Hwan",
title="On the value of outlier elimination on software effort estimation research",
journal="Empirical Software Engineering",
year="2013",
month="Aug",
day="01",
volume="18",
number="4",
pages="659--698",
abstract="Producing accurate and reliable software effort estimation has always been a challenge for both academic research and software industries. Regarding this issue, data quality is an important factor that impacts the estimation accuracy of effort estimation methods. To assess the impact of data quality, we investigated the effect of eliminating outliers on the estimation accuracy of commonly used software effort estimation methods. Based on three research questions, we associatively analyzed the influence of outlier elimination on the accuracy of software effort estimation by applying five methods of outlier elimination (Least trimmed squares, Cook's distance, K-means clustering, Box plot, and Mantel leverage metric) and two methods of effort estimation (Least squares regression and Estimation by analogy with the variation of the parameters). Empirical experiments were performed using industrial data sets (ISBSG Release 9, Bank and Stock data sets that are collected from financial companies, and a Desharnais data set in the PROMISE repository). In addition, the effect of the outlier elimination methods is evaluated by the statistical tests (the Friedman test and the Wilcoxon signed rank test). The experimental results derived from the evaluation criteria showed that there was no substantial difference between the software effort estimation results with and without outlier elimination. However, statistical analysis indicated that outlier elimination leads to a significant improvement in the estimation accuracy on the Stock data set (in case of some combinations of outlier elimination and effort estimation methods). In addition, although outlier elimination did not lead to a significant improvement in the estimation accuracy on the other data sets, our graphical analysis of errors showed that outlier elimination can improve the likelihood to produce more accurate effort estimates for new software project data to be estimated. Therefore, from a practical point of view, it is necessary to consider the outlier elimination and to conduct a detailed analysis of the effort estimation results to improve the accuracy of software effort estimation in software organizations.",
issn="1573-7616",
doi="10.1007/s10664-012-9207-y",
url="https://doi.org/10.1007/s10664-012-9207-y"
}

@Article{Kou2009,
author="Kou, Hongbing
and Johnson, Philip M.
and Erdogmus, Hakan",
title="Operational definition and automated inference of test-driven development with Zorro",
journal="Automated Software Engineering",
year="2009",
month="Nov",
day="10",
volume="17",
number="1",
pages="57",
abstract="Test-driven development (TDD) is a style of development named for its most visible characteristic: the design and implementation of test cases prior to the implementation of the code required to make them pass. Many claims have been made for TDD: that it can improve implementation as well as design quality, that it can improve productivity, that it results in 100{\%} coverage, and so forth. However, research to validate these claims has yielded mixed and sometimes contradictory results. We believe that at least part of the reason for these results stems from differing interpretations of the TDD development style, along with an inability to determine whether programmers actually follow whatever definition of TDD is in use.",
issn="1573-7535",
doi="10.1007/s10515-009-0058-8",
url="https://doi.org/10.1007/s10515-009-0058-8"
}

@Article{Karlsson2007,
author="Karlsson, Lena
and Thelin, Thomas
and Regnell, Bj{\"o}rn
and Berander, Patrik
and Wohlin, Claes",
title="Pair-wise comparisons versus planning game partitioning---experiments on requirements prioritisation techniques",
journal="Empirical Software Engineering",
year="2007",
month="Feb",
day="01",
volume="12",
number="1",
pages="3--33",
abstract="The process of selecting the right set of requirements for a product release is dependent on how well the organisation succeeds in prioritising the requirements candidates. This paper describes two consecutive controlled experiments comparing different requirements prioritisation techniques with the objective of understanding differences in time-consumption, ease of use and accuracy. The first experiment evaluates Pair-wise comparisons and a variation of the Planning game. As the Planning game turned out as superior, the second experiment was designed to compare the Planning game to Tool-supported pair-wise comparisons. The results indicate that the manual pair-wise comparisons is the most time-consuming of the techniques, and also the least easy to use. Tool-supported pair-wise comparisons is the fastest technique and it is as easy to use as the Planning game. The techniques do not differ significantly regarding accuracy.",
issn="1573-7616",
doi="10.1007/s10664-006-7240-4",
url="https://doi.org/10.1007/s10664-006-7240-4"
}

@Article{Maldonado2006,
author="Maldonado, Jos{\'e} C.
and Carver, Jeffrey
and Shull, Forrest
and Fabbri, Sandra
and D{\'o}ria, Emerson
and Martimiano, Luciana
and Mendon{\c{c}}a, Manoel
and Basili, Victor",
title="Perspective-Based Reading: A Replicated Experiment Focused on Individual Reviewer Effectiveness",
journal="Empirical Software Engineering",
year="2006",
month="Mar",
day="01",
volume="11",
number="1",
pages="119--142",
abstract="This paper describes a replication conducted to compare the effectiveness of inspectors using Perspective Based Reading (PBR) to the effectiveness of inspectors using a checklist. The goal of this replication was to better understand the complementary aspects of the PBR perspectives. To this end, a brief discussion of the original study is provided as well as a more detailed description of the replication. A detailed statistical analysis is then provided along with analysis of the PBR perspectives.",
issn="1573-7616",
doi="10.1007/s10664-006-5967-6",
url="https://doi.org/10.1007/s10664-006-5967-6"
}

@Article{GarcíaGuzmán2013,
author="Garc{\'i}a Guzm{\'a}n, Javier
and Mart{\'i}n, Diego
and Urbano, Juli{\'a}n
and de Amescua, Antonio",
title="Practical experiences in modelling software engineering practices: The project patterns approach",
journal="Software Quality Journal",
year="2013",
month="Jun",
day="01",
volume="21",
number="2",
pages="325--354",
abstract="Software process improvement in software development organisations is a complex task that can be solved using knowledge management strategies. In this area, the definition and use of process patterns are a proven approach to apply knowledge management strategies in software engineering organisations. One of the main problems for the effective application of process patterns in the software industry is the difficulty of formalising the knowledge about the development process using these approaches. This study presents a framework to manage software project patterns. This framework (which is composed of a metamodel and a platform for patterns modelling and reuse) is able to formalise the knowledge on software development projects including software engineers' previous experience, development methodologies, references frameworks and lessons learnt. The authors carried out an empirical study at Carlos III University of Madrid, where junior software engineers used the project patterns defined in this research work. The evidences and findings obtained during the empirical study execution indicates that correctness of the pattern depends on relevance of the bibliographic references used to create it, implementation of a knowledge sharing strategy among the personnel involved and previous experience in the business areas related to the information systems being developed. The results obtained from the empirical study also envisage that the usefulness of an sdPP (Software Development Project Pattern) depends on the ease of identifying when and how to apply a specific sdPP in a software project.",
issn="1573-1367",
doi="10.1007/s11219-012-9177-8",
url="https://doi.org/10.1007/s11219-012-9177-8"
}

@Article{Caglayan2015,
author="Caglayan, Bora
and Tosun Misirli, Ayse
and Bener, Ayse Basar
and Miranskyy, Andriy",
title="Predicting defective modules in different test phases",
journal="Software Quality Journal",
year="2015",
month="Jun",
day="01",
volume="23",
number="2",
pages="205--227",
abstract="Defect prediction is a well-established research area in software engineering. Prediction models in the literature do not predict defect-prone modules in different test phases. We investigate the relationships between defects and test phases in order to build defect prediction models for different test phases. We mined the version history of a large-scale enterprise software product to extract churn and static code metrics. We used three testing phases that have been employed by our industry partner, namely function, system and field, to build a learning-based model for each testing phase. We examined the relation of different defect symptoms with the testing phases. We compared the performance of our proposed model with a benchmark model that has been constructed for the entire test phase (benchmark model). Our results show that building a model to predict defect-prone modules for each test phase significantly improves defect prediction performance and shortens defect detection time. The benefit analysis shows that using the proposed model, the defects are detected on the average 7 months earlier than the actual. The outcome of prediction models should lead to an action in a software development organization. Our proposed model gives a more granular outcome in terms of predicting defect-prone modules in each testing phase so that managers may better organize the testing teams and effort.",
issn="1573-1367",
doi="10.1007/s11219-014-9230-x",
url="https://doi.org/10.1007/s11219-014-9230-x"
}

@Article{Deiters1998,
author="Deiters, Wolfgang
and Gruhn, Volker",
title="Process Management in Practice Applying the FUNSOFT Net Approach to Large-Scale Processes",
journal="Automated Software Engineering",
year="1998",
month="Jan",
day="01",
volume="5",
number="1",
pages="7--25",
abstract="Management of business and software processes are areas of increasing interest, which evolved nearly independently from each other. In this article we present an approach to process management that has been applied to business and software processes and which, thereby, enabled cross-fertilization between both areas. The goal of this article is to report lessons learned in industrial as well as academic business and software process management projects.",
issn="1573-7535",
doi="10.1023/A:1008654224389",
url="https://doi.org/10.1023/A:1008654224389"
}

@Article{Henderson-Sellers2002,
author="Henderson-Sellers, Brian",
title="Process Metamodelling and Process Construction: Examples Using the OPEN Process Framework (OPF)",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="341--362",
abstract="Deriving a unique software development process is not possible since the requirements of individual projects vary significantly. What is possible is a standard framework, defined by a metamodel, which can then provide an extensible and tailorable process environment such that individual and project-specific processes can be created and configured precisely to those project needs. As an illustration of this approach, the OPEN Process Framework (OPF) is described in terms of its underpinning metamodel. The use of this framework to construct individual process instances is then described, first in principle and then illustrated by three case studies: a process for mid-sized MIS development; a process for Web development; and a process to assist organizations in transitioning from a non-OO to an OO development culture.",
issn="1573-7489",
doi="10.1023/A:1020570027891",
url="https://doi.org/10.1023/A:1020570027891"
}

@Article{Harrison1997,
author="Harrison, R.
and Briand, L.
and Daly, J.
and Kellner, M.
and Raffo, D. M.
and Shepperd, M. J.",
title="Process Modelling and Empirical Studies of Software Evolution (PMESSE'97) Workshop Report",
journal="Empirical Software Engineering",
year="1997",
month="Dec",
day="01",
volume="2",
number="4",
pages="381--403",
abstract="Much progress is being made in both the areas of process modelling and software metrics. However, neither of these concepts is complete without the other: processes cannot be improved if no assessment of quality is available, and metrics are useless if they cannot be applied in order to assess the evolution of systems. The PMESSE (Process Modelling and Empirical Studies of Software Evolution) Workshop, held in Boston MA, on May 18, 1997, brought together researchers and practitioners from both of these fields, and stimulated some very lively debate on these issues. This collection of reports reflects the work done by the Workshops five Working Groups.",
issn="1573-7616",
doi="10.1023/A:1009749918745",
url="https://doi.org/10.1023/A:1009749918745"
}

@Article{Francis1993,
author="Francis, Richard",
title="Quality and process management: a view from the UK computing services industry",
journal="Software Quality Journal",
year="1993",
month="Dec",
day="01",
volume="2",
number="4",
pages="225--238",
abstract="This paper presents the main approaches (ISO9000, TickIT, SPICE, Process Improvement and TQM) along the road to quality improvement. It has been distilled from a technical note Quality and process management programmes: approaches and experience issued earlier this year by the Computing Services Association, the UK trade association of computing software and service companies whose members account for three-quarters of turnover of the UK industry. CSA member company experience is included as case studies of particular approaches, describing the background to the approach taken and the main benefits gained. A discussion on the role of metrics within any such improvement option is included, as is a report on a recent well-attended CSA seminar on the subject.",
issn="1573-1367",
doi="10.1007/BF00403765",
url="https://doi.org/10.1007/BF00403765"
}

@Article{Mohagheghi2007,
author="Mohagheghi, Parastoo
and Conradi, Reidar",
title="Quality, productivity and economic benefits of software reuse: a review of industrial studies",
journal="Empirical Software Engineering",
year="2007",
month="Oct",
day="01",
volume="12",
number="5",
pages="471--516",
abstract="Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges.",
issn="1573-7616",
doi="10.1007/s10664-007-9040-x",
url="https://doi.org/10.1007/s10664-007-9040-x"
}

@Article{Ellis2013,
author="Ellis, Keith
and Berry, Daniel M.",
title="Quantifying the impact of requirements definition and management process maturity on project outcome in large business application development",
journal="Requirements Engineering",
year="2013",
month="Sep",
day="01",
volume="18",
number="3",
pages="223--249",
abstract="Using data from two surveys of people knowledgeable about requirements for, and the success of the development of, large commercial applications (CAs) in hundreds of large organizations from around the world, this paper reports a high positive correlation between an organization's requirements definition and management (RDM) maturity and that organization's successful performance on CA development projects. Among the organizations that responded with a filled survey, an organization that is assessed at a high RDM maturity is significantly more successful in its CA development projects than is an organization that is assessed at a low RDM maturity, when success in CA development projects is measured as (1) delivering CAs on-time, on-budget, and on-function, (2) meeting the business objectives of these projects, and (3) the perceived success of these projects. This paper presents a comprehensive framework for RDM, describes a quality RDM process, and describes RDM maturity and how to measure it. It describes the two surveys, the first of which ended up being a pilot for the second, which was designed taking into account what was learned from the first survey. The paper concludes with advice to practitioners on the application of the RDM maturity framework in any organization that wishes to improve its RDM and its performance in the development of large CAs.",
issn="1432-010X",
doi="10.1007/s00766-012-0146-3",
url="https://doi.org/10.1007/s00766-012-0146-3"
}

@Article{Kitchenham2010,
author="Kitchenham, Barbara A.
and Brereton, Pearl
and Turner, Mark
and Niazi, Mahmood K.
and Linkman, Stephen
and Pretorius, Rialette
and Budgen, David",
title="Refining the systematic literature review process---two participant-observer case studies",
journal="Empirical Software Engineering",
year="2010",
month="Dec",
day="01",
volume="15",
number="6",
pages="618--653",
abstract="Systematic literature reviews (SLRs) are a major tool for supporting evidence-based software engineering. Adapting the procedures involved in such a review to meet the needs of software engineering and its literature remains an ongoing process. As part of this process of refinement, we undertook two case studies which aimed 1) to compare the use of targeted manual searches with broad automated searches and 2) to compare different methods of reaching a consensus on quality. For Case 1, we compared a tertiary study of systematic literature reviews published between January 1, 2004 and June 30, 2007 which used a manual search of selected journals and conferences and a replication of that study based on a broad automated search. We found that broad automated searches find more studies than manual restricted searches, but they may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers, or they are assessing research trends in research methodologies. For Case 2, we analyzed the process used to evaluate the quality of SLRs. We conclude that if quality evaluation of primary studies is a critical component of a specific SLR, assessments should be based on three independent evaluators incorporating at least two rounds of discussion.",
issn="1573-7616",
doi="10.1007/s10664-010-9134-8",
url="https://doi.org/10.1007/s10664-010-9134-8"
}

@Article{Jedlitschka2014,
author="Jedlitschka, Andreas
and Juristo, Natalia
and Rombach, Dieter",
title="Reporting experiments to satisfy professionals' information needs",
journal="Empirical Software Engineering",
year="2014",
month="Dec",
day="01",
volume="19",
number="6",
pages="1921--1955",
abstract="Although the aim of empirical software engineering is to provide evidence for selecting the appropriate technology, it appears that there is a lack of recognition of this work in industry. Results from empirical research only rarely seem to find their way to company decision makers. If information relevant for software managers is provided in reports on experiments, such reports can be considered as a source of information for them when they are faced with making decisions about the selection of software engineering technologies. To bridge this communication gap between researchers and professionals, we propose characterizing the information needs of software managers in order to show empirical software engineering researchers which information is relevant for decision-making and thus enable them to make this information available. We empirically investigated decision makers' information needs to identify which information they need to judge the appropriateness and impact of a software technology. We empirically developed a model that characterizes these needs. To ensure that researchers provide relevant information when reporting results from experiments, we extended existing reporting guidelines accordingly. We performed an experiment to evaluate our model with regard to its effectiveness. Software managers who read an experiment report according to the proposed model judged the technology's appropriateness significantly better than those reading a report about the same experiment that did not explicitly address their information needs. Our research shows that information regarding a technology, the context in which it is supposed to work, and most importantly, the impact of this technology on development costs and schedule as well as on product quality is crucial for decision makers.",
issn="1573-7616",
doi="10.1007/s10664-013-9268-6",
url="https://doi.org/10.1007/s10664-013-9268-6"
}

@Article{Martínez-Ruiz2012,
author="Mart{\'i}nez-Ruiz, Tom{\'a}s
and M{\"u}nch, J{\"u}rgen
and Garc{\'i}a, F{\'e}lix
and Piattini, Mario",
title="Requirements and constructors for tailoring software processes: a systematic literature review",
journal="Software Quality Journal",
year="2012",
month="Mar",
day="01",
volume="20",
number="1",
pages="229--260",
abstract="Organizations developing software-based systems or services often need to tailor process reference models---including product-oriented and project-oriented processes---to meet both their own characteristics and those of their projects. Existing process reference models, however, are often defined in a generic manner. They typically offer only limited mechanisms for adapting processes to the needs of organizational units, project goals, and project environments. This article presents a systematic literature review of peer-reviewed conference and journal articles published between 1990 and 2009. Our aim was both to identify requirements for process-tailoring notation and to analyze those tailoring mechanisms that are currently in existence and that consistently support process tailoring. The results show that the software engineering community has demonstrated an ever-increasing interest in software process tailoring, ranging from the consideration of theoretical proposals regarding how to tailor processes to the scrutiny of practical experiences in organizations. Existing tailoring mechanisms principally permit the modeling of variations of activities, artifacts, or roles by insertion or deletion. Two types of variations have been proposed: the individual modification of process elements and the simultaneous variation of several process elements. Resolving tailoring primarily refers to selecting or deselecting optional elements or to choosing between alternatives. It is sometimes guided by explicitly defined processes and supported by tools or mechanisms from the field of knowledge engineering. The study results show that tailoring notations are not as mature as the industry requires if they are to provide the kind of support for process tailoring that fulfills the requirements identified, i.e., including security policies for the whole process, or carrying out one activity rather than another. A notation must therefore be built, which takes these requirements into consideration in order to permit variant-rich processes representation and use this variability to consistently support process tailoring.",
issn="1573-1367",
doi="10.1007/s11219-011-9147-6",
url="https://doi.org/10.1007/s11219-011-9147-6"
}

@Article{Damian2005,
author="Damian, Daniela
and Chisan, James
and Vaidyanathasamy, Lakshminarayanan
and Pal, Yogendra",
title="Requirements Engineering and Downstream Software Development: Findings from a Case Study",
journal="Empirical Software Engineering",
year="2005",
month="Jul",
day="01",
volume="10",
number="3",
pages="255--283",
abstract="Requirements management is being recognized as one of the most important albeit difficult phases in software engineering. The literature repeatedly cites the role of well-defined requirements and requirements management process in problem analysis and project management as benefiting software development throughout the life cycle: during design, coding, testing, maintenance and documentation of software. This paper reports on the findings of an investigation into industrial practice of requirements management process improvement and its positive effects on downstream software development. The evidence reveals a strong relationship between a well-defined requirements process and increased developer productivity, improved project planning through better estimations and enhanced ability for stakeholders to negotiate project scope. These results are important since there is little empirical evidence of the actual benefits of sound requirements practice, in spite of the plethora of claims in the literature. An account of these effects not only adds to our understanding of good requirements practice but also provides strong motivation for software organizations to develop programs for improvement of their requirements processes.",
issn="1573-7616",
doi="10.1007/s10664-005-1288-4",
url="https://doi.org/10.1007/s10664-005-1288-4"
}

@Article{McCaffery2009,
author="Mc Caffery, Fergal
and Burton, John
and Richardson, Ita",
title="Risk management capability model for the development of medical device software",
journal="Software Quality Journal",
year="2009",
month="Sep",
day="22",
volume="18",
number="1",
pages="81",
abstract="Failure of medical device (MD) software can have potentially catastrophic effects, leading to injury of patients or even death. Therefore, regulators penalise MD manufacturers who do not demonstrate that sufficient attention is devoted to the areas of hazard analysis and risk management (RM) throughout the software lifecycle. This paper has two main objectives. The first objective is to compare how thorough current MD regulations are with relation to the Capability Maturity Model Integration (CMMI®) in specifying what RM practices MD companies should adopt when developing software. The second objective is to present a Risk Management Capability Model (RMCM) for the MD software industry, which is geared towards improving software quality, safety and reliability. Our analysis indicates that 42 RM sub-practices would have to be performed in order to satisfy MD regulations and that only an additional 8 sub-practices would be required in order to satisfy all the CMMI® level 1 requirements. Additionally, MD companies satisfying the CMMI® goals of the RM process area by performing the CMMI® RM practices will not meet the requirements of the MD software RM regulations as an additional 20 MD-specific sub-practices have to be added to meet the objectives of RMCM.",
issn="1573-1367",
doi="10.1007/s11219-009-9086-7",
url="https://doi.org/10.1007/s11219-009-9086-7"
}


@Article{Felderer2016,
author="Felderer, Michael
and Ramler, Rudolf",
title="Risk orientation in software testing processes of small and medium enterprises: an exploratory and comparative study",
journal="Software Quality Journal",
year="2016",
month="Sep",
day="01",
volume="24",
number="3",
pages="519--548",
abstract="Risk orientation in testing is an important means to balance quality, time-to-market, and cost of software. Especially for small and medium enterprises (SME) under high competitive and economic pressure, risk orientation can help to focus testing activities on critical areas of a software product. Although several risk-based approaches to testing are available, the topic has so far not been investigated in the context of SME, where risks are often associated with business critical issues. This article fills the gap and explores the state of risk orientation in the testing processes of SME. Furthermore, it compares the state of risk-based testing in SME to the situation in large enterprises. The article is based on a multiple case study conducted with five SME. A previous study on risk-based testing in large enterprises is used as reference for investigating the differences between risk orientation in SME and large enterprises. The findings of our study show that a strong business focus, the use of informal risk concepts, as well as the application of risk knowledge to reduce testing cost and time are key differences of risk-based testing in SME compared to large enterprises.",
issn="1573-1367",
doi="10.1007/s11219-015-9289-z",
url="https://doi.org/10.1007/s11219-015-9289-z"
}

@Article{Lehman2001,
author="Lehman, Meir M.
and Ramil, Juan F.",
title="Rules and Tools for Software Evolution Planning and Management",
journal="Annals of Software Engineering",
year="2001",
month="Nov",
day="01",
volume="11",
number="1",
pages="15--44",
abstract="When first formulated in the early seventies, the laws of software evolution were, for a number of reasons, not widely accepted as relevant to software engineering practice. Over the years, however, they have gradually become recognised as providing useful inputs to understanding of the software process. Now eight in number, they have been supplemented by the software uncertainty principle and the FEAST (Feedback, Evolution And Software Technology) hypothesis. Based on all these and on the further results of the FEAST research projects this paper develops and presents some fifty rules for application in software system process planning and management and indicates tools available or that could usefully be developed to support their application. The listing is structured according to the laws that encapsulate the observed phenomena and that lead to the recommendations. Each sublist is preceded by a textual discussion providing at least some of the reasoning that has led to the recommended procedures. The references direct the interested reader to the literature that records observed behaviours, interpretations, models and metrics obtained from industrially evolved systems, and from which the recommendations were derived.",
issn="1573-7489",
doi="10.1023/A:1012535017876",
url="https://doi.org/10.1023/A:1012535017876"
}

@Article{Grundy1998,
author="Grundy, John C.
and Hosking, John G.",
title="Serendipity: Integrated Environment Support for Process Modelling, Enactment and Work Coordination",
journal="Automated Software Engineering",
year="1998",
month="Jan",
day="01",
volume="5",
number="1",
pages="27--60",
abstract="Large cooperative work systems require work coordination, context awareness and process modelling and enactment mechanisms to be effective. Support for process modelling and work coordination in such systems also needs to support informal aspects of work which are difficult to codify. Computer-Supported Cooperative Work (CSCW) facilities, such as inter-person communication and collaborative editing, also need to be well-integrated into both process-modelling tools and tools used to perform work. Serendipity is an environment which provides high-level, visual process modelling and event-handling languages, and diverse CSCW capabilities, and which can be integrated with a range of tools to coordinate cooperative work. This paper describes Serendipity's visual languages, support environment, architecture, and implementation, together with experience using the environment and integrating it with other environments.",
issn="1573-7535",
doi="10.1023/A:1008606308460",
url="https://doi.org/10.1023/A:1008606308460"
}

@Article{Leung1999,
author="Leung, Hareton K. N.",
title="Slow Change of Information System Development Practice",
journal="Software Quality Journal",
year="1999",
month="Nov",
day="01",
volume="8",
number="3",
pages="197--210",
abstract="Recent advances in information system (IS) development such as object oriented technology, CASE tools, and increasing emphasis on software quality should bring better products and faster delivery to the customer. Yet, despite the high expectation of these new technologies, it is not clear whether there is any real change at the working level. This paper presents a multi-year study on the change of development practice, quality practice, and project performance of the software industry. Data on various dimensions of IS development was collected in three consecutive years. The results indicated that the changes were slow over the period. In many areas, no significant change was detected, except there was a slightly increasing use of CASE tools. It seems that the lack of resources, short term outlook, and unknown value of process improvement may be the contributing factors to the slow change.",
issn="1573-1367",
doi="10.1023/A:1008915509865",
url="https://doi.org/10.1023/A:1008915509865"
}

@Article{Lehman2002,
author="Lehman, Meir M.
and Ramil, Juan F.",
title="Software Evolution and Software Evolution Processes",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="275--309",
abstract="Most of the software in regular use in businesses and organisations all over the world cannot be completely specified. It cannot be implemented, once and for all. Both the original implementation and the inevitable subsequent evolution (maintenance) are a continual learning experience driven, inter alia, by feedback from the results of the behaviour under execution of the software, as perceived by various stakeholders, by advances and growth in the user organisations and by adaptation to changes in the external world, both independent and as a result of installation and use of the software. Real world, termed type-E, software is essentially evolutionary in nature. The study of the processes of evolution of such software is of considerable interest, as is that of the domains that co-evolve with the software. After briefly discussing the meaning of the term evolution in the context of software, its technology, the software process and related domains, this paper describes some of the facets of the evolution phenomenon and implications to the evolution process as identified during many years of active interest in the topic.",
issn="1573-7489",
doi="10.1023/A:1020557525901",
url="https://doi.org/10.1023/A:1020557525901"
}

@Article{Pino2008,
author="Pino, Francisco J.
and Garc{\'i}a, F{\'e}lix
and Piattini, Mario",
title="Software process improvement in small and medium software enterprises: a systematic review",
journal="Software Quality Journal",
year="2008",
month="Jun",
day="01",
volume="16",
number="2",
pages="237--261",
abstract="Small and medium enterprises are a very important cog in the gears of the world economy. The software industry in most countries is composed of an industrial scheme that is made up mainly of small and medium software enterprises---SMEs. To strengthen these types of organizations, efficient Software Engineering practices are needed---practices which have been adapted to their size and type of business. Over the last two decades, the Software Engineering community has expressed special interest in software process improvement (SPI) in an effort to increase software product quality, as well as the productivity of software development. However, there is a widespread tendency to make a point of stressing that the success of SPI is only possible for large companies. In this article, a systematic review of published case studies on the SPI efforts carried out in SMEs is presented. Its objective is to analyse the existing approaches towards SPI which focus on SMEs and which report a case study carried out in industry. A further objective is that of discussing the significant issues related to this area of knowledge, and to provide an up-to-date state of the art, from which innovative research activities can be thought of and planned.",
issn="1573-1367",
doi="10.1007/s11219-007-9038-z",
url="https://doi.org/10.1007/s11219-007-9038-z"
}

@Article{Iqbal2016,
author="Iqbal, Javed
and Ahmad, Rodina Binti
and Nasir, Mohd Hairul Nizam Md
and Niazi, Mahmood
and Shamshirband, Shahaboddin
and Noor, Muhammad Asim",
title="Software SMEs' unofficial readiness for CMMI®-based software process improvement",
journal="Software Quality Journal",
year="2016",
month="Dec",
day="01",
volume="24",
number="4",
pages="997--1023",
abstract="The goal of software process improvement (SPI) is to improve software processes and produce high-quality software, but the results of SPI efforts in small- and medium-sized enterprises (SMEs) that develop software have been unsatisfactory. The objective of this study is to support the prolific and successful CMMI-based implementation of SPI in SMEs by presenting the facts related to the unofficial adoption of CMMI level 2 process area-specific practices by software SMEs. Two questionnaire surveys were performed, and 42 questionnaires were selected for data analysis. The questionnaires were filled out by experts from 42 non-CMMI-certified software SMEs based in Malaysia and Pakistan. In the case of each process area of CMMI level 2, the respondents were asked to choose from three categories, namely `below 50 {\%},' `50--75 {\%},' and `above 75 {\%}'. The percentages indicated the extent to which process area-specific practices are routinely followed in the respondents' respective organizations. To deal with differing standards for defining SMEs, the notion of the common range standard has been introduced. The results of the study show that a large segment of software development SMEs informally follows the specific practices of CMMI level 2 process areas and thus has true potential for rapid and effective CMMI-based SPI. The results further indicate that, in the case of four process areas of CMMI level 2, there are statistically significant differences between the readiness of small and medium software enterprises to adopt the specific practices of those process areas, and between trends on their part to do so unofficially. The findings, manifesting various degrees of unofficial readiness for CMMI-based SPI among SMEs, can be used to define criteria for the selection of SMEs that would be included in SPI initiatives funded by relevant authorities. In the interests of developing fruitful CMMI-based SPI and to enhance the success rate of CMMI-based SPI initiatives, the study suggests that `ready' or `potential' SMEs should be given priority for SPI initiatives.",
issn="1573-1367",
doi="10.1007/s11219-015-9277-3",
url="https://doi.org/10.1007/s11219-015-9277-3"
}

@Article{Baggen2012,
author="Baggen, Robert
and Correia, Jos{\'e} Pedro
and Schill, Katrin
and Visser, Joost",
title="Standardized code quality benchmarking for improving software maintainability",
journal="Software Quality Journal",
year="2012",
month="Jun",
day="01",
volume="20",
number="2",
pages="287--307",
abstract="We provide an overview of the approach developed by the Software Improvement Group for code analysis and quality consulting focused on software maintainability. The approach uses a standardized measurement model based on the ISO/IEC 9126 definition of maintainability and source code metrics. Procedural standardization in evaluation projects further enhances the comparability of results. Individual assessments are stored in a repository that allows any system at hand to be compared to the industry-wide state of the art in code quality and maintainability. When a minimum level of software maintainability is reached, the certification body of T{\"U}V Informationstechnik GmbH issues a Trusted Product Maintainability certificate for the software product.",
issn="1573-1367",
doi="10.1007/s11219-011-9144-9",
url="https://doi.org/10.1007/s11219-011-9144-9"
}

@Article{Pikkarainen2012,
author="Pikkarainen, Minna
and Salo, Outi
and Kuusela, Raija
and Abrahamsson, Pekka",
title="Strengths and barriers behind the successful agile deployment---insights from the three software intensive companies in Finland",
journal="Empirical Software Engineering",
year="2012",
month="Dec",
day="01",
volume="17",
number="6",
pages="675--702",
abstract="The number of success stories being reported concerning agile software development has led to an increase in interest among industries and research communities. The purpose of this paper is to identify strengths and barriers for `successful agile deployment' in the software companies. This knowledge can benefit software companies planning their current strategy for agile deployment. Analysis of 57 developers, architects, project managers, customers, quality managers, and line and product managers in three case companies identifies 71 strengths and 169 barriers of agile deployment. The analysis revealed the importance of management providing the necessary goals and support for agile development. It also indicated the significance of defining a tailored process model and giving developers the freedom to improve their own agile development process continuously during agile deployment. The identified barriers, strengths and recommendations can be used as a checklist for planning and/or monitoring the effectiveness of agile deployment in software companies. By identifying the barriers and strengths of agile deployment, the paper deepens understanding of this highly relevant but relatively under-researched phenomenon and contributes to the literature on agile deployment and software process improvement.",
issn="1573-7616",
doi="10.1007/s10664-011-9185-5",
url="https://doi.org/10.1007/s10664-011-9185-5"
}

@Article{Lethbridge2005,
author="Lethbridge, Timothy C.
and Sim, Susan Elliott
and Singer, Janice",
title="Studying Software Engineers: Data Collection Techniques for Software Field Studies",
journal="Empirical Software Engineering",
year="2005",
month="Jul",
day="01",
volume="10",
number="3",
pages="311--341",
abstract="Software engineering is an intensively people-oriented activity, yet too little is known about how designers, maintainers, requirements analysts and all other types of software engineers perform their work. In order to improve software engineering tools and practice, it is therefore essential to conduct field studies, i.e. to study real practitioners as they solve real problems. To do so effectively, however, requires an understanding of the techniques most suited to each type of field study task. In this paper, we provide a taxonomy of techniques, focusing on those for data collection. The taxonomy is organized according to the degree of human intervention each requires. For each technique, we provide examples from the literature, an analysis of some of its advantages and disadvantages, and a discussion of how to use it effectively. We also briefly talk about field study design in general, and data analysis.",
issn="1573-7616",
doi="10.1007/s10664-005-1290-x",
url="https://doi.org/10.1007/s10664-005-1290-x"
}

@Article{Hubaux2013,
author="Hubaux, Arnaud
and Heymans, Patrick
and Schobbens, Pierre-Yves
and Deridder, Dirk
and Abbasi, Ebrahim Khalil",
title="Supporting multiple perspectives in feature-based configuration",
journal="Software {\&} Systems Modeling",
year="2013",
month="Jul",
day="01",
volume="12",
number="3",
pages="641--663",
abstract="Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.",
issn="1619-1374",
doi="10.1007/s10270-011-0220-1",
url="https://doi.org/10.1007/s10270-011-0220-1"
}

@Article{Osei-Bryson2004,
author="Osei-Bryson, Kweku-Muata
and Ngwenyama, Ojelanki K.",
title="Supporting Semantic Diversity in the Relational Data Model: The Case of Multi-Face Attributes",
journal="Information Systems Frontiers",
year="2004",
month="Sep",
day="01",
volume="6",
number="3",
pages="277--285",
abstract="In recent years various proposals have been offered for increasing the richness of the relational data model by addressing specific user requirements, particularly with regard to structural and behavioral expressiveness. Although there have been some criticisms of the semantic limitations of the model, few proposals have emerged to address them. In this paper we propose an extension of the model to address some of the semantic limitations around the use of multi-face attributes in everyday activity. We present a formal discussion of multi-face attributes and suggest how they can be accommodated in the relational model and relational database software systems. The resulting model offers a higher-level expressiveness and will provide the user with increased flexibility in the input, output and storage of data, and the specification of queries. Finally we outline some of the implications for database design methods that emerge for the extension.",
issn="1572-9419",
doi="10.1023/B:ISFI.0000037881.10525.07",
url="https://doi.org/10.1023/B:ISFI.0000037881.10525.07"
}

@Article{Catal2013,
author="Catal, Cagatay
and Mishra, Deepti",
title="Test case prioritization: a systematic mapping study",
journal="Software Quality Journal",
year="2013",
month="Sep",
day="01",
volume="21",
number="3",
pages="445--478",
abstract="Test case prioritization techniques, which are used to improve the cost-effectiveness of regression testing, order test cases in such a way that those cases that are expected to outperform others in detecting software faults are run earlier in the testing phase. The objective of this study is to examine what kind of techniques have been widely used in papers on this subject, determine which aspects of test case prioritization have been studied, provide a basis for the improvement of test case prioritization research, and evaluate the current trends of this research area. We searched for papers in the following five electronic databases: IEEE Explorer, ACM Digital Library, Science Direct, Springer, and Wiley. Initially, the search string retrieved 202 studies, but upon further examination of titles and abstracts, 120 papers were identified as related to test case prioritization. There exists a large variety of prioritization techniques in the literature, with coverage-based prioritization techniques (i.e., prioritization in terms of the number of statements, basic blocks, or methods test cases cover) dominating the field. The proportion of papers on model-based techniques is on the rise, yet the growth rate is still slow. The proportion of papers that use datasets from industrial projects is found to be 64 {\%}, while those that utilize public datasets for validation are only 38 {\%}. On the basis of this study, the following recommendations are provided for researchers: (1) Give preference to public datasets rather than proprietary datasets; (2) develop more model-based prioritization methods; (3) conduct more studies on the comparison of prioritization methods; (4) always evaluate the effectiveness of the proposed technique with well-known evaluation metrics and compare the performance with the existing methods; (5) publish surveys and systematic review papers on test case prioritization; and (6) use datasets from industrial projects that represent real industrial problems.",
issn="1573-1367",
doi="10.1007/s11219-012-9181-z",
url="https://doi.org/10.1007/s11219-012-9181-z"
}

@Article{Dogša2011,
author="Dog{\v{s}}a, Toma{\v{z}}
and Bati{\v{c}}, David",
title="The effectiveness of test-driven development: an industrial case study",
journal="Software Quality Journal",
year="2011",
month="Dec",
day="01",
volume="19",
number="4",
pages="643--661",
abstract="Test-driven development (TDD) is a software development practice, where test cases are incrementally written before implementing the production code. This paper presents the results of a multi-case study investigating the effectiveness of TDD within an industrial environment. Three comparable medium-sized projects were observed during their development cycle. Two projects were driven without TDD practice, while the third one introduced TDD into the development process. The effectiveness of TDD was expressed in terms of external code quality, productivity, and maintainability. Our results indicate that the TDD developers produced higher quality code that is easier to maintain, although we did observe a reduction in productivity.",
issn="1573-1367",
doi="10.1007/s11219-011-9130-2",
url="https://doi.org/10.1007/s11219-011-9130-2"
}

@Article{Pikkarainen2008,
author="Pikkarainen, M.
and Haikara, J.
and Salo, O.
and Abrahamsson, P.
and Still, J.",
title="The impact of agile practices on communication in software development",
journal="Empirical Software Engineering",
year="2008",
month="Jun",
day="01",
volume="13",
number="3",
pages="303--337",
abstract="Agile software development practices such as eXtreme Programming (XP) and SCRUM have increasingly been adopted to respond to the challenges of volatile business environments, where the markets and technologies evolve rapidly and present the unexpected. In spite of the encouraging results so far, little is known about how agile practices affect communication. This article presents the results from a study which examined the impact of XP and SCRUM practices on communication within software development teams and within the focal organization. The research was carried out as a case study in F-Secure where two agile software development projects were compared from the communication perspective. The goal of the study is to increase the understanding of communication in the context of agile software development: internally among the developers and project leaders and in the interface between the development team and stakeholders (i.e. customers, testers, other development teams). The study shows that agile practices improve both informal and formal communication. However, it further indicates that, in larger development situations involving multiple external stakeholders, a mismatch of adequate communication mechanisms can sometimes even hinder the communication. The study highlights the fact that hurdles and improvements in the communication process can both affect the feature requirements and task subtask dependencies as described in coordination theory. While the use of SCRUM and some XP practices facilitate team and organizational communication of the dependencies between product features and working tasks, the use of agile practices requires that the team and organization use also additional plan-driven practices to ensure the efficiency of external communication between all the actors of software development.",
issn="1573-7616",
doi="10.1007/s10664-008-9065-9",
url="https://doi.org/10.1007/s10664-008-9065-9"
}

@Article{Fusaro1998,
author="Fusaro, Pierfrancesco
and Emam, Khaled El
and Smith, Bob",
title="The Internal Consistencies of the 1987 SEI Maturity Questionnaire and the SPICE Capability Dimension",
journal="Empirical Software Engineering",
year="1998",
month="Jun",
day="01",
volume="3",
number="2",
pages="179--201",
abstract="This paper presents the results of an empirical evaluation of the reliability of two commonly used assessment instruments: the 1987 SEI Maturity Questionnaire and the SPICE v1 capability dimension. The type of reliability that was evaluated is internal consistency. A study of the internal consistency of the 1987 questionnaire was only briefly mentioned in a 1991 article, and the internal consistency of the SPICE v1 capability dimension has not been evaluated thus far. We used two different data sets to evaluate the internal consistency of each instrument. Our results indicate that both assessment instruments are very reliable and also have similar reliability levels. The results are encouraging for users of assessment instruments, and provide a baseline with which to compare subsequent versions of these instruments.",
issn="1573-7616",
doi="10.1023/A:1008036316053",
url="https://doi.org/10.1023/A:1008036316053"
}

@Article{Al-Baik2015,
author="Al-Baik, Osama
and Miller, James",
title="The kanban approach, between agility and leanness: a systematic review",
journal="Empirical Software Engineering",
year="2015",
month="Dec",
day="01",
volume="20",
number="6",
pages="1861--1897",
abstract="The interest in lean product development in general and the Kanban approach in particular has increased over the years. However, practitioners, in the software development field, have significant challenges in implementing the Kanban approach as it lacks a clear definition of its principles, practices, techniques and tools. This study aims to provide insight into the Kanban approach and its elements (concepts, principles, practices, techniques, and tools) that have been empirically reported by scholars and practitioners. This insight is produced by using the systematic review method to analyze the available literature. A total of 37 primary studies were selected from more than 3,000 unique studies. Our findings show that the primary studies have considered and reported 20 different elements as part of the Kanban approach based upon considerations of being an agile approach or a lean principle; these elements have realized great benefits and improvements to the software development teams. These benefits along with the challenges have been reported in this study. Due to the variety of organization types, contexts, and project sizes reported in the primary studies, it is expected that the results in this study would help in establishing knowledge on what are the different elements of the Kanban approach as well as offering a first step towards developing guidelines for practitioners to help in introducing the Kanban approach to software development organizations.",
issn="1573-7616",
doi="10.1007/s10664-014-9340-x",
url="https://doi.org/10.1007/s10664-014-9340-x"
}

@Article{Bjarnason2017,
author="Bjarnason, Elizabeth
and Sharp, Helen",
title="The role of distances in requirements communication: a case study",
journal="Requirements Engineering",
year="2017",
month="Mar",
day="01",
volume="22",
number="1",
pages="1--26",
abstract="Requirements communication plays a vital role in development projects in coordinating the customers, the business roles and the software engineers. Communication gaps represent a significant source of project failures and overruns. For example, misunderstood or uncommunicated requirements can lead to software that does not meet the customers' requirements, and subsequent low number of sales or additional cost required to redo the implementation. We propose that requirements engineering (RE) distance measures are useful for locating gaps in requirements communication and for improving on development practice. In this paper, we present a case study of one software development project to evaluate this proposition. Thirteen RE distances were measured including geographical and cognitive distances between project members, and semantic distances between requirements and testing artefacts. The findings confirm that RE distances impact requirements communication and project coordination. Furthermore, the concept of distances was found to enable constructive group reflection on communication gaps and improvements to development practices. The insights reported in this paper can provide practitioners with an increased awareness of distances and their impact. Furthermore, the results provide a stepping stone for further research into RE distances and methods for improving on software development processes and practices.",
issn="1432-010X",
doi="10.1007/s00766-015-0233-3",
url="https://doi.org/10.1007/s00766-015-0233-3"
}

@Article{Prechelt2011,
author="Prechelt, Lutz
and Oezbek, Christopher",
title="The search for a research method for studying OSS process innovation",
journal="Empirical Software Engineering",
year="2011",
month="Aug",
day="01",
volume="16",
number="4",
pages="514--537",
abstract="Medium-sized, open-participation Open Source Software (OSS) projects do not usually perform explicit software process improvement on any routine basis. It would be useful to understand how to get such a project to accept a process improvement proposal and hence to perform process innovation. We want to determine an effective and feasible qualitative research method for studying the above question. We present (narratively) a case study of how we worked towards and eventually found such a research method. The case involves four attempts at collecting suitable data about innovation episodes (direct participation (twice), polling developers for episodes, manually finding episodes in mailing list archives) and the adaptation of the Grounded Theory data analysis methodology. Direct participation allows gathering rather rich data, but does not allow for observing a sufficiently large number of innovation episodes. Polling developers for episodes did not prove to be useful. Using mailing list archives to find data to be analyzed is both feasible and effective. We also describe how the data thus found can be analyzed based on the Grounded Theory Method with suitable adjustments. By-and-large, our findings ought to apply to studying various phenomena in OSS development processes that are similarly heavyweight and infrequent. However, specific details may block this possibility and we cannot predict which details that might be. The amount of effort involved in direct participation approaches to qualitative research can easily be underestimated. Also, survey approaches are not well-suited for many process issues in OSS, because too few developers are sufficiently process-conscious. An approach based on passive observation is a viable alternative in the OSS context due to the availability of large amounts of fairly complete archival data.",
issn="1573-7616",
doi="10.1007/s10664-011-9160-1",
url="https://doi.org/10.1007/s10664-011-9160-1"
}

@Article{Villela2005,
author="Villela, Karina
and Santos, Gleison
and Schnaider, L{\'i}lian
and Rocha, Ana Regina
and Travassos, Guilherme Horta",
title="The use of an enterprise ontology to support knowledge management in software development environments",
journal="Journal of the Brazilian Computer Society",
year="2005",
month="Jun",
day="01",
volume="11",
number="2",
pages="45--59",
abstract="Software engineering is knowledge-intensive activity and knowledge is thought to be the most important asset in an organization. Therefore this paper presents an approach to support Knowledge Management in Software Development Environments that is strongly based on ontologies: Enterprise Oriented Software Development Environments. After describing the components of such environments, this paper focuses on the Enterprise Ontology and on three tools developed based on this ontology: a `yellow pages' tool which shows the distribution of competencies in the organization, a tool to support the allocation of people to software projects and a graphic tool for representing and visualizing organizational processes.",
issn="1678-4804",
doi="10.1007/BF03192375",
url="https://doi.org/10.1007/BF03192375"
}

@Article{Robinson1996,
author="Robinson, K.
and Simmons, P.",
title="The value of a certified quality management system: the perception of internal developers",
journal="Software Quality Journal",
year="1996",
month="Jun",
day="01",
volume="5",
number="2",
pages="61--73",
abstract="There is growing interest among internal information technology (IT) departments in implementing a certified quality management system (QMS). This paper examines the forces that have influenced organizations that supply development services to adopt QMS models and explores the relevance of these forces to internal IT developers. The perceptions of internal developers regarding the value of QMS in addressing these forces were studied using a multiple case study involving four organizations. The results suggest that while the reasons given by external developers concern mainly the marketing advantages of adopting a QMS, internal developers envisage the major advantage to be an improvement in the development process.",
issn="1573-1367",
doi="10.1007/BF00419770",
url="https://doi.org/10.1007/BF00419770"
}

@Article{Storey2006,
author="Storey, Margaret-Anne",
title="Theories, tools and research methods in program comprehension: past, present and future                  ",
journal="Software Quality Journal",
year="2006",
month="Sep",
day="01",
volume="14",
number="3",
pages="187--208",
abstract="Program comprehension research can be characterized by both the theories that provide rich explanations about how programmers understand software, as well as the tools that are used to assist in comprehension tasks. In this paper, I review some of the key cognitive theories of program comprehension that have emerged over the past thirty years. Using these theories as a canvas, I then explore how tools that are commonly used today have evolved to support program comprehension. Specifically, I discuss how the theories and tools are related and reflect on the research methods that were used to construct the theories and evaluate the tools. The reviewed theories and tools are distinguished according to human characteristics, program characteristics, and the context for the various comprehension tasks. Finally, I predict how these characteristics will change in the future and speculate on how a number of important research directions could lead to improvements in program comprehension tool development and research methods.",
issn="1573-1367",
doi="10.1007/s11219-006-9216-4",
url="https://doi.org/10.1007/s11219-006-9216-4"
}

@Article{Humphrey2002,
author="Humphrey, Watts S.",
title="Three Process Perspectives: Organizations, Teams, and People",
journal="Annals of Software Engineering",
year="2002",
month="Dec",
day="01",
volume="14",
number="1",
pages="39--72",
abstract="This paper provides the author's personal views and perspectives on software process improvement. Starting with his first work on technology assessment in IBM over 20 years ago, Watts Humphrey describes the process improvement work he has been directly involved in. This includes the development of the early process assessment methods, the original design of the CMM, and the introduction of the Personal Software Process (PSP)SM and Team Software Process (TSP){\{}SM{\}}. In addition to describing the original motivation for this work, the author also reviews many of the problems he and his associates encountered and why they solved them the way they did. He also comments on the outstanding issues and likely directions for future work. Finally, this work has built on the experiences and contributions of many people. Mr. Humphrey only describes work that he was personally involved in and he names many of the key contributors. However, so many people have been involved in this work that a full list of the important participants would be impractical.",
issn="1573-7489",
doi="10.1023/A:1020593305601",
url="https://doi.org/10.1023/A:1020593305601"
}

@Article{Hoxmeier1998,
author="Hoxmeier, John A.",
title="Typology of database quality factors",
journal="Software Quality Journal",
year="1998",
month="Sep",
day="01",
volume="7",
number="3",
pages="179--193",
abstract="Databases are a critical element of virtually all conventional and ebusiness applications. How does an organization know if the information derived from the database is any good? To ensure a quality database application, should the emphasis during model development be on the application of quality assurance metrics (designing it right)? A large number of database applications fail or are unusable. A quality process does not necessarily lead to a usable database product. A database application can also be `well-formed' with high data quality but lack semantic or cognitive fidelity (the right design). This paper expands on the growing body of literature in the area of data quality by proposing additions to a hierarchy of database quality dimensions that includes model and behavioral factors in addition to process and data factors.",
issn="1573-1367",
doi="10.1023/A:1008923120973",
url="https://doi.org/10.1023/A:1008923120973"
}

@Article{Benestad2010,
author="Benestad, Hans Christian
and Anda, Bente
and Arisholm, Erik",
title="Understanding cost drivers of software evolution: a quantitative and qualitative investigation of change effort in two evolving software systems",
journal="Empirical Software Engineering",
year="2010",
month="Apr",
day="01",
volume="15",
number="2",
pages="166--203",
abstract="Making changes to software systems can prove costly and it remains a challenge to understand the factors that affect the costs of software evolution. This study sought to identify such factors by investigating the effort expended by developers to perform 336 change tasks in two different software organizations. We quantitatively analyzed data from version control systems and change trackers to identify factors that correlated with change effort. In-depth interviews with the developers about a subset of the change tasks further refined the analysis. Two central quantitative results found that dispersion of changed code and volatility of the requirements for the change task correlated with change effort. The analysis of the qualitative interviews pointed to two important, underlying cost drivers: Difficulties in comprehending dispersed code and difficulties in anticipating side effects of changes. This study demonstrates a novel method for combining qualitative and quantitative analysis to assess cost drivers of software evolution. Given our findings, we propose improvements to practices and development tools to manage and reduce the costs.",
issn="1573-7616",
doi="10.1007/s10664-009-9118-8",
url="https://doi.org/10.1007/s10664-009-9118-8"
}

@Article{Sánchez-Gordón2016,
author="S{\'a}nchez-Gord{\'o}n, Mary-Luz
and O'Connor, Rory V.",
title="Understanding the gap between software process practices and actual practice in very small companies",
journal="Software Quality Journal",
year="2016",
month="Sep",
day="01",
volume="24",
number="3",
pages="549--570",
abstract="This paper reports on a grounded theory to study into software developers' use of software development processes in actual practice in the specific context of very small companies. This study was conducted in three very small software product companies located in Ecuador. The data collection was based on semi-structured qualitative interviews with software project managers, focus group with software developers and was supplemented by the literature and document studies. We interviewed two types of participants (managers and developers), so as to ensure that we elicited a holistic perspective of how they approached the software development process in actual practice. The goal was to study what practices are actually used and their opinion and attitude toward the potential adopting of an international standard (ISO/IEC 29110) specifically designed for very small companies. With the collected data, we performed an analysis utilizing grounded theory coding techniques, as this methodology promotes the focus on uncovering the real concerns of the participants. This study highlighted three areas of concern: customer, software product and development tasks coordination and tracking. The findings in this study give an insight toward the work products as they relate to software development process practices in very small companies and the important factors that must be considered to assist project success.",
issn="1573-1367",
doi="10.1007/s11219-015-9282-6",
url="https://doi.org/10.1007/s11219-015-9282-6"
}

@Article{Assar2016,
author="Assar, Sa{\"i}d
and Borg, Markus
and Pfahl, Dietmar",
title="Using text clustering to predict defect resolution time: a conceptual replication and an evaluation of prediction accuracy",
journal="Empirical Software Engineering",
year="2016",
month="Aug",
day="01",
volume="21",
number="4",
pages="1437--1475",
abstract="Defect management is a central task in software maintenance. When a defect is reported, appropriate resources must be allocated to analyze and resolve the defect. An important issue in resource allocation is the estimation of Defect Resolution Time (DRT). Prior research has considered different approaches for DRT prediction exploiting information retrieval techniques and similarity in textual defect descriptions. In this article, we investigate the potential of text clustering for DRT prediction. We build on a study published by Raja (2013) which demonstrated that clusters of similar defect reports had statistically significant differences in DRT. Raja's study also suggested that this difference between clusters could be used for DRT prediction. Our aims are twofold: First, to conceptually replicate Raja's study and to assess the repeatability of its results in different settings; Second, to investigate the potential of textual clustering of issue reports for DRT prediction with focus on accuracy. Using different data sets and a different text mining tool and clustering technique, we first conduct an independent replication of the original study. Then we design a fully automated prediction method based on clustering with a simulated test scenario to check the accuracy of our method. The results of our independent replication are comparable to those of the original study and we confirm the initial findings regarding significant differences in DRT between clusters of defect reports. However, the simulated test scenario used to assess our prediction method yields poor results in terms of DRT prediction accuracy. Although our replication confirms the main finding from the original study, our attempt to use text clustering as the basis for DRT prediction did not achieve practically useful levels of accuracy.",
issn="1573-7616",
doi="10.1007/s10664-015-9391-7",
url="https://doi.org/10.1007/s10664-015-9391-7"
}

@Article{Kalanidhi2001,
author="Kalanidhi, Sanjeev",
title="Value Creation in a Network: The Role of Pricing and Revenue Optimization and Enterprise Profit OptimizationTM",
journal="Information Systems Frontiers",
year="2001",
month="Dec",
day="01",
volume="3",
number="4",
pages="465--470",
abstract="The Internet has created a virtual upheaval in the structural features of the supply and demand chains for most businesses. New agents and marketplaces have surfaced. The potential to create value and enhance profitable opportunities has attracted both buyers and sellers to the Internet. Yet, the Internet has proven to be more complex than originally thought. With information comes complexity: the more the information in real time, the greater the difficulty in interpretation and absorption. How can the value-creating potential of the Internet still be realized, its complexity notwithstanding?",
issn="1572-9419",
doi="10.1023/A:1012828921804",
url="https://doi.org/10.1023/A:1012828921804"
}

@Article{Münch2005,
author="M{\"u}nch, J{\"u}rgen
and Pfahl, Dietmar
and Rus, Ioana",
title="Virtual Software Engineering Laboratories in Support of Trade-off Analyses",
journal="Software Quality Journal",
year="2005",
month="Dec",
day="01",
volume="13",
number="4",
pages="407",
abstract="Due to demanding customer needs and evolving technology, software organizations are forced to trade individual functional and non-functional product quality profiles against other factors such as cost, time, or productivity. The ability to influence or even control these factors requires a deep understanding of the complex relations between process and product attributes in relevant contexts. Based on such understanding, decision support is needed to adjust processes so that they match the product quality goals without violating given project constraints. We propose to use a Virtual Software Engineering Laboratory (VSEL) to establish such decision support cost-effectively. VSELs can be considered as being complementary to existing (empirical) Software Engineering Laboratories. This paper gives an introduction into the cornerstones of VSELs, discusses how they complement traditional empirically based Software Engineering Laboratories (SELs), and illustrates with the help of case examples from industrial and research environments, how to use them in support of product-focused trade-off analyses.",
issn="1573-1367",
doi="10.1007/s11219-005-4253-y",
url="https://doi.org/10.1007/s11219-005-4253-y"
}

@Article{Casey2010,
author="Casey, Valentine",
title="Virtual software team project management",
journal="Journal of the Brazilian Computer Society",
year="2010",
month="Aug",
day="01",
volume="16",
number="2",
pages="83--96",
abstract="Globally distributed information systems development has become a key strategy for large sections of the software industry. This involves outsourcing projects to third parties or offshoring development to divisions in remote locations. A popular approach when implementing these strategies is the establishment of virtual teams. The justification for embarking on this approach is to endeavor to leverage the potential benefits of labor arbitrage available between geographical locations. When implementing such a strategy organizations must recognize that virtual teams operate differently to collocated teams, therefore, they must be managed differently. These differences arise due to the complex and collaborative nature of information systems development and the impact distance introduces. Geographical, temporal, cultural, and linguistic distance all negatively impact on coordination, cooperation, communication, and visibility in the virtual team setting. In these circumstances, it needs to be recognized that the project management of a virtual team must be carried out in a different manner to that of a collocated team. Results from this research highlight six specific project management areas, which need to be addressed to facilitate successful virtual team operation. They are: Organizational Virtual Team Strategy, Risk Management, Infrastructure, Implementation of a Virtual Team Process, Team Structure and Organization, and Conflict Management.",
issn="1678-4804",
doi="10.1007/s13173-010-0013-3",
url="https://doi.org/10.1007/s13173-010-0013-3"
}

@Article{Al-Baik2014,
author="Al-Baik, Osama
and Miller, James",
title="Waste identification and elimination in information technology organizations",
journal="Empirical Software Engineering",
year="2014",
month="Dec",
day="01",
volume="19",
number="6",
pages="2019--2061",
abstract="In this paper, we propose a new model to classify wastes in IT organizations. In the beginning, we discuss the potential reasons behind the relatively low success rate of lean initiatives in knowledge-based industries in general and in IT organizations in particular. The virtual nature of business processes in IT organizations calls into question the applicability of Toyota's categorization of physical wastes in IT settings. Then, through a real-life project, we develop a new model of waste categorization for the operation of a ``medium-sized'' IT department. In addition to the new classifications specific to IT, we discuss suitable elimination strategies and how they have improved the daily operations of the organization by reducing the lead-time by 56--60 {\%}, increasing customer satisfactions by 15.7 {\%}, and saving hundreds of thousands of the operational cost. Finally, we emphasize the need to think lean when developing the waste elimination strategies by eliminating the root cause of the waste and not the subsequent wastes.",
issn="1573-7616",
doi="10.1007/s10664-014-9302-3",
url="https://doi.org/10.1007/s10664-014-9302-3"
}

@Article{Mackie1997,
author="Mackie, C.",
title="Process excellence and capability determination",
journal="BT Technology Journal",
year="1997",
month="Jul",
day="01",
volume="15",
number="3",
pages="130--139",
abstract="BT is faced with a constant challenge to deliver quality software against increasing customer demands. It is important to understand the processes in use and to be able to continually improve them. The first step was the adoption of ISO9001 and then ISO9000-3 (TickIT)) within BT. These standards were, however, while useful, considered too prescriptive to fully meet our needs. A new standard, ISO 15504 also known as SPICE, moves us forward from the prescriptive audit method towards a flexible assessment approach. SPICE provides a measure of the maturity of a process against a defined model of best practice, rather than a pass/fail result as with ISO9001. This paper presents an overview of the emerging SPICE standard, together with a discussion on its potential benefits for BT.",
issn="1573-1995",
doi="10.1023/A:1018646521455",
url="https://doi.org/10.1023/A:1018646521455"
}

